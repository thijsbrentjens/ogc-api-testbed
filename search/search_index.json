{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"OGC-API-Testbed Documentation This documents the Geonovum OGC API Testbed Platform. Focus is on the Platform's design/setup, how it is provisioned (bootstrapped) and its continuous deployment/integration (CI/CD). Initially, the main goal of the Platform is to experiment with, and evaluate various implementations of the OGC API Features (OAFeat) Standard . Given the generic nature of the Platform's web-services deployment architecture, additional services and OGC APIs may be added. The stable version of the Platform is provided as an Open Source GitHub Template , allowing any party to derive and customize their own. Find a quick intro in the project README . The documentation is split up as follows: Setup describes the platform architecture and setup (admin manual) HOWTO has a number of tutorials on how to operate the system (user manual) Findings design choices, identified challenges and solutions Cases contains some experiments performed on the platform, and may be extended to capture future outcomes of the testbed Get in Touch Services To access and interact with the (OGC web-) services, go to one of the two server instances: Stable (production) server at apitestbed.geonovum.nl Sandbox (experimental) server at apisandbox.geonovum.nl Links Project GitHub Repo Geonovum - Geonovum home pygeoapi - pygeoapi project home ldproxy - ldproxy project home GeoServer - GeoServer home OGC API Features - OGC Home OAFeat","title":"Home"},{"location":"#ogc-api-testbed-documentation","text":"This documents the Geonovum OGC API Testbed Platform. Focus is on the Platform's design/setup, how it is provisioned (bootstrapped) and its continuous deployment/integration (CI/CD). Initially, the main goal of the Platform is to experiment with, and evaluate various implementations of the OGC API Features (OAFeat) Standard . Given the generic nature of the Platform's web-services deployment architecture, additional services and OGC APIs may be added. The stable version of the Platform is provided as an Open Source GitHub Template , allowing any party to derive and customize their own. Find a quick intro in the project README . The documentation is split up as follows: Setup describes the platform architecture and setup (admin manual) HOWTO has a number of tutorials on how to operate the system (user manual) Findings design choices, identified challenges and solutions Cases contains some experiments performed on the platform, and may be extended to capture future outcomes of the testbed","title":"OGC-API-Testbed Documentation"},{"location":"#get-in-touch","text":"","title":"Get in Touch"},{"location":"#services","text":"To access and interact with the (OGC web-) services, go to one of the two server instances: Stable (production) server at apitestbed.geonovum.nl Sandbox (experimental) server at apisandbox.geonovum.nl","title":"Services"},{"location":"#links","text":"Project GitHub Repo Geonovum - Geonovum home pygeoapi - pygeoapi project home ldproxy - ldproxy project home GeoServer - GeoServer home OGC API Features - OGC Home OAFeat","title":"Links"},{"location":"cases/","text":"Cases A number of specific experiments have been carried out. INSPIRE Case API Strategie Case Extending OGC API Features Skinning OGC API Features","title":"Cases"},{"location":"cases/#cases","text":"A number of specific experiments have been carried out. INSPIRE Case API Strategie Case Extending OGC API Features Skinning OGC API Features","title":"Cases"},{"location":"cases/INSPIRE/","text":"INSPIRE findings The INSPIRE community has described an approach to provide INSPIRE Download services using OGC API Features . This document reviews this approach for various products used in the testbed. Similar to Atom the Main principles of the approach indicate to set up a single api endpoint for this dataset. Both GeoServer and LDProxy offer capabilitiy to set up multiple endpoint within a single service, for pygeoapi we have to set up a new service. Requirements class \u201cINSPIRE-pre-defined-data-set-download-OAPIF\u201d MRCO Aspect pygeoapi GeoServer LDProxy Comment M Supports OpenApi 3.0 + + + GeoHealthCheck found issues in GeoServer and pygeoapi M /collections has metadata link for dataset + - ? pygeoapi is flexible for configuring any type of links C If HTML endoding, /collections has metadata link as html + - ? C For harmonised datasets, on collection level a should be included to feature concept dictionary + - ? R For harmonised datasets, collectionid should match featuretype from IR + ? ? M /colections has link to license + - ? R License information in accordance with openapi + - - OpenAPI fields info/termsOfService or info/license are mentioned M Mandatory, C Conditional, R Recommended, O Optional Requirements class INSPIRE-multilinguality All aspects are conditional, in case the dataset is multilingual. pygeoapi landed a multiligual feature recently, other products seem to not have multilingual capabilities. MRCO Aspect pygeoapi GeoServer LDProxy Comment C Support accept-language header + - - R Behaviour on no matching lang B B B Returns default language C Content language header + - - R Language support at all paths + - - M hreflang on enclosure links + - - M Mandatory, C Conditional, R Recommended, O Optional Requirements class \u201cINSPIRE-OAPIF-GeoJSON\u201d MRCO Aspect pygeoapi GeoServer LDProxy Comment R document encoding rules to geojson ? ? ? no efforts yet M Mandatory, C Conditional, R Recommended, O Optional Requirements class \u201cINSPIRE-bulk-download\u201d MRCO Aspect pygeoapi GeoServer LDProxy Comment M link to entire dataset + - - M link has type from inspire mediatypes + - - R link has length attribute + - - R link has title attribute + - - M Mandatory, C Conditional, R Recommended, O Optional Requirements class \u201cINSPIRE-CRS\u201d MRCO Aspect pygeoapi GeoServer LDProxy Comment R at least 1 supported CRS from list + + + M Mandatory, C Conditional, R Recommended, O Optional","title":"INSPIRE findings"},{"location":"cases/INSPIRE/#inspire-findings","text":"The INSPIRE community has described an approach to provide INSPIRE Download services using OGC API Features . This document reviews this approach for various products used in the testbed. Similar to Atom the Main principles of the approach indicate to set up a single api endpoint for this dataset. Both GeoServer and LDProxy offer capabilitiy to set up multiple endpoint within a single service, for pygeoapi we have to set up a new service.","title":"INSPIRE findings"},{"location":"cases/INSPIRE/#requirements-class-inspire-pre-defined-data-set-download-oapif","text":"MRCO Aspect pygeoapi GeoServer LDProxy Comment M Supports OpenApi 3.0 + + + GeoHealthCheck found issues in GeoServer and pygeoapi M /collections has metadata link for dataset + - ? pygeoapi is flexible for configuring any type of links C If HTML endoding, /collections has metadata link as html + - ? C For harmonised datasets, on collection level a should be included to feature concept dictionary + - ? R For harmonised datasets, collectionid should match featuretype from IR + ? ? M /colections has link to license + - ? R License information in accordance with openapi + - - OpenAPI fields info/termsOfService or info/license are mentioned M Mandatory, C Conditional, R Recommended, O Optional","title":"Requirements class \u201cINSPIRE-pre-defined-data-set-download-OAPIF\u201d"},{"location":"cases/INSPIRE/#requirements-class-inspire-multilinguality","text":"All aspects are conditional, in case the dataset is multilingual. pygeoapi landed a multiligual feature recently, other products seem to not have multilingual capabilities. MRCO Aspect pygeoapi GeoServer LDProxy Comment C Support accept-language header + - - R Behaviour on no matching lang B B B Returns default language C Content language header + - - R Language support at all paths + - - M hreflang on enclosure links + - - M Mandatory, C Conditional, R Recommended, O Optional","title":"Requirements class INSPIRE-multilinguality"},{"location":"cases/INSPIRE/#requirements-class-inspire-oapif-geojson","text":"MRCO Aspect pygeoapi GeoServer LDProxy Comment R document encoding rules to geojson ? ? ? no efforts yet M Mandatory, C Conditional, R Recommended, O Optional","title":"Requirements class \u201cINSPIRE-OAPIF-GeoJSON\u201d"},{"location":"cases/INSPIRE/#requirements-class-inspire-bulk-download","text":"MRCO Aspect pygeoapi GeoServer LDProxy Comment M link to entire dataset + - - M link has type from inspire mediatypes + - - R link has length attribute + - - R link has title attribute + - - M Mandatory, C Conditional, R Recommended, O Optional","title":"Requirements class \u201cINSPIRE-bulk-download\u201d"},{"location":"cases/INSPIRE/#requirements-class-inspire-crs","text":"MRCO Aspect pygeoapi GeoServer LDProxy Comment R at least 1 supported CRS from list + + + M Mandatory, C Conditional, R Recommended, O Optional","title":"Requirements class \u201cINSPIRE-CRS\u201d"},{"location":"cases/api_rules/","text":"API Strategie Findings The knowledge Platform API's has published a normative document on REST-API design rules . This document explains how you can set up an OGC API service respecting these rules. ID Aspect Comment API-01 Adhere to HTTP safety and idempotency semantics for operations API-02 Do not maintain session state on the server API-03 Only apply standard HTTP methods API-04 Define interfaces in Dutch unless there is an official English glossary available API-05 Use nouns to name resources Collections and items are user by name API-06 Use nested URIs for child resources Items are children of collections API-10 Model resource operations as a sub-resource or dedicated resource API-16 Use OpenAPI Specification for documentation API-17 Publish documentation in Dutch unless there is existing documentation in English API-18 Include a deprecation schedule when publishing API changes API-19 Schedule a fixed transition period for a new major API version API-20 Include the major version number in the URI API-48 Leave off trailing slashes from URIs API-51 Publish OAS document at a standard location in JSON-format API-53 Hide irrelevant implementation details API-54 Use plural nouns to name collection resources API-55 Publish a changelog for API changes between versions API-56 Adhere to the Semantic Versioning model when releasing API changes API-57 Return the full version number in a response header","title":"API Strategie Findings"},{"location":"cases/api_rules/#api-strategie-findings","text":"The knowledge Platform API's has published a normative document on REST-API design rules . This document explains how you can set up an OGC API service respecting these rules. ID Aspect Comment API-01 Adhere to HTTP safety and idempotency semantics for operations API-02 Do not maintain session state on the server API-03 Only apply standard HTTP methods API-04 Define interfaces in Dutch unless there is an official English glossary available API-05 Use nouns to name resources Collections and items are user by name API-06 Use nested URIs for child resources Items are children of collections API-10 Model resource operations as a sub-resource or dedicated resource API-16 Use OpenAPI Specification for documentation API-17 Publish documentation in Dutch unless there is existing documentation in English API-18 Include a deprecation schedule when publishing API changes API-19 Schedule a fixed transition period for a new major API version API-20 Include the major version number in the URI API-48 Leave off trailing slashes from URIs API-51 Publish OAS document at a standard location in JSON-format API-53 Hide irrelevant implementation details API-54 Use plural nouns to name collection resources API-55 Publish a changelog for API changes between versions API-56 Adhere to the Semantic Versioning model when releasing API changes API-57 Return the full version number in a response header","title":"API Strategie Findings"},{"location":"cases/extending/","text":"Extending pygeoapi An interesting use case around OGC API's is the ability to extend a base product with additional methods to facilitate more advanced data interaction. For example on a dataset with 'public announcements', citizens may want to interact with an announcement by sharing it with their friends, upvote or comment on it. Of all products in the testbed pygeoapi seems most appropriate to be extended to facilitate this use case. Unfortunately pygeoapi currently does not offer any extension points to add new methods. There is however the OGC API Processes endpoint which can be used for this purpose. Also I provide an example of an extension point in pygeoapi, to indicate how extensions are managed in pygeoapi. OGC API processes OGC API processes has a similar goal, to extend interacting with datasets by offering the capability to run processes on a dataset. The advantage is that users don't need to download the data, but can interact with the data at its origin. Processes are defined server side. Which processes are available is listed in the /processes endpoint. OGC API processes is more verbose then what people expect in modern api's. For example submitting 2 parameters to the hello-world process requires this input json object. { \"inputs\": [ { \"id\": \"name\", \"type\": \"text/plain\", \"value\": \"World\" }, { \"id\": \"message\", \"type\": \"text/plain\", \"value\": \"An optional message.\" } ] } Above data structure facilitates quite complex input parameters and is well described in the open api document. An important benefit is that this is a standadised client-server interaction. Both synchronous and asynchronous cases are supported by OGC API Processes. Read more on how processes can be defined in pygeoapi Extending pygeoapi pygeoapi has been developed with the idea of running it standalone or as a library in for example GeoNode. The optimal way to extend pygeoapi is to create a dedicated project and add pygeoapi as a dependency to the project. Extending pygeoapi is currently most common in the provider section. Users are invited to write their own providr plugin which manages access to a dedicated backend. An example of this is https://github.com/Canadian-Geospatial-Platform/geocore-pygeoapi. The project is a provider plugin to access a dedicated spatial catalogue backend in order to provide an OGC API Records layer by pygeoapi. The project includes pygeoapi as a depenendy (via requirements.txt ).","title":"Extending pygeoapi"},{"location":"cases/extending/#extending-pygeoapi","text":"An interesting use case around OGC API's is the ability to extend a base product with additional methods to facilitate more advanced data interaction. For example on a dataset with 'public announcements', citizens may want to interact with an announcement by sharing it with their friends, upvote or comment on it. Of all products in the testbed pygeoapi seems most appropriate to be extended to facilitate this use case. Unfortunately pygeoapi currently does not offer any extension points to add new methods. There is however the OGC API Processes endpoint which can be used for this purpose. Also I provide an example of an extension point in pygeoapi, to indicate how extensions are managed in pygeoapi.","title":"Extending pygeoapi"},{"location":"cases/extending/#ogc-api-processes","text":"OGC API processes has a similar goal, to extend interacting with datasets by offering the capability to run processes on a dataset. The advantage is that users don't need to download the data, but can interact with the data at its origin. Processes are defined server side. Which processes are available is listed in the /processes endpoint. OGC API processes is more verbose then what people expect in modern api's. For example submitting 2 parameters to the hello-world process requires this input json object. { \"inputs\": [ { \"id\": \"name\", \"type\": \"text/plain\", \"value\": \"World\" }, { \"id\": \"message\", \"type\": \"text/plain\", \"value\": \"An optional message.\" } ] } Above data structure facilitates quite complex input parameters and is well described in the open api document. An important benefit is that this is a standadised client-server interaction. Both synchronous and asynchronous cases are supported by OGC API Processes. Read more on how processes can be defined in pygeoapi","title":"OGC API processes"},{"location":"cases/extending/#extending-pygeoapi_1","text":"pygeoapi has been developed with the idea of running it standalone or as a library in for example GeoNode. The optimal way to extend pygeoapi is to create a dedicated project and add pygeoapi as a dependency to the project. Extending pygeoapi is currently most common in the provider section. Users are invited to write their own providr plugin which manages access to a dedicated backend. An example of this is https://github.com/Canadian-Geospatial-Platform/geocore-pygeoapi. The project is a provider plugin to access a dedicated spatial catalogue backend in order to provide an OGC API Records layer by pygeoapi. The project includes pygeoapi as a depenendy (via requirements.txt ).","title":"Extending pygeoapi"},{"location":"cases/skinning/","text":"Skinning OGC API Features HTML is a first class citizen in OGC API Common. This means that a typical OGC API can be accessed via a web browser, offering a human readable interface. This aspect brings in a usability aspect that providers previously didn't need to worry about. Aspects such as corporate identity, WCAG (accessibility), search engine optimisation or a cookie/privacy statement. For the experiment we want to understand how easy it is to update basic aspects on the html visualisation in various OGC API products. pygeoapi pygeoapi uses jinja templates for html output. These templates are located at ~/pygeoapi/templates . You can override these templates at their location. But you can also set a separate template override folder , where you can place (a part of the) updated templates. Updating the templates requires basic html skills, subsituted parameters are placed in curly braces: <footer class=\"sticky\">Powered by <a title=\"pygeoapi\" href=\"https://pygeoapi.io\"> <img src=\"{{ config['server']['url'] }}/static/img/pygeoapi.png\" title=\"pygeoapi logo\" style=\"height:24px;vertical-align: middle;\"/> </a> {{ version }} </footer> pycsw The implementation of OGC API Records in pycsw is derived from the pygeoapi implementation. The templates are located at ~/pycsw/ogc/api/templates . QGIS QGIS uses similar jinja templates as pygeoapi, you can override the resources folder via the environment variable QGIS_SERVER_API_RESOURCES_DIRECTORY. The standard location of the templates is ~/qgis/resources/server/api/ogc/templates/wfs3 . GeoServer GeoServer uses Freemarker templates to render content in html. These .ftl files are persisted within .jar files. A basic override approach could be to extract gs-ogcapi-features.jar to a folder, adjust the templates, and zip the package back to a .jar file and deploy it. GeoServer also provides a template override mechanism from the data folder. Read more at a dedicated blog on this topic . Freemarker uses a similar substitution mechanism as jinja: <li>Mail: <a href=\"mailto:${contact.contactEmail}\">${contact.contactEmail}</a></li> To highlight the impact of skinning, we have prepared a tailored skin for geoserver at https://apitestbed.geonovum.nl/geoserver/ogc/features. We've added a layout inspired on the GeoNovum website. For this, 2 layout template overrides have been added to ~/data/templates/ogc/features . LDProxy The HTML encoding is implemented using Mustache templates . Custom templates are supported, they have to reside in the data directory under the relative path templates/html/{templateName}.mustache , where {templateName} equals the name of a default template (see source code on GitHub) (taken from ldproxy docs ). GeoNetwork GeoNetwork offers an experimental OGC API Records implementation at https://github.com/geonetwork/geonetwork-microservices/tree/main/modules/services/ogc-api-records. This plugin can be installed on the latest v4 GeoNetwork. GeoNetwork uses xslt to provide a html interface. The xslt templates are located at https://github.com/geonetwork/geonetwork-microservices/tree/main/modules/services/ogc-api-records/src/main/resources/xslt/ogcapir. <div class=\"w-2/3 pr-4\"> <abstract> <xsl:value-of select=\"$abstract\"/> </abstract> </div>","title":"Skinning OGC API Features"},{"location":"cases/skinning/#skinning-ogc-api-features","text":"HTML is a first class citizen in OGC API Common. This means that a typical OGC API can be accessed via a web browser, offering a human readable interface. This aspect brings in a usability aspect that providers previously didn't need to worry about. Aspects such as corporate identity, WCAG (accessibility), search engine optimisation or a cookie/privacy statement. For the experiment we want to understand how easy it is to update basic aspects on the html visualisation in various OGC API products.","title":"Skinning OGC API Features"},{"location":"cases/skinning/#pygeoapi","text":"pygeoapi uses jinja templates for html output. These templates are located at ~/pygeoapi/templates . You can override these templates at their location. But you can also set a separate template override folder , where you can place (a part of the) updated templates. Updating the templates requires basic html skills, subsituted parameters are placed in curly braces: <footer class=\"sticky\">Powered by <a title=\"pygeoapi\" href=\"https://pygeoapi.io\"> <img src=\"{{ config['server']['url'] }}/static/img/pygeoapi.png\" title=\"pygeoapi logo\" style=\"height:24px;vertical-align: middle;\"/> </a> {{ version }} </footer>","title":"pygeoapi"},{"location":"cases/skinning/#pycsw","text":"The implementation of OGC API Records in pycsw is derived from the pygeoapi implementation. The templates are located at ~/pycsw/ogc/api/templates .","title":"pycsw"},{"location":"cases/skinning/#qgis","text":"QGIS uses similar jinja templates as pygeoapi, you can override the resources folder via the environment variable QGIS_SERVER_API_RESOURCES_DIRECTORY. The standard location of the templates is ~/qgis/resources/server/api/ogc/templates/wfs3 .","title":"QGIS"},{"location":"cases/skinning/#geoserver","text":"GeoServer uses Freemarker templates to render content in html. These .ftl files are persisted within .jar files. A basic override approach could be to extract gs-ogcapi-features.jar to a folder, adjust the templates, and zip the package back to a .jar file and deploy it. GeoServer also provides a template override mechanism from the data folder. Read more at a dedicated blog on this topic . Freemarker uses a similar substitution mechanism as jinja: <li>Mail: <a href=\"mailto:${contact.contactEmail}\">${contact.contactEmail}</a></li> To highlight the impact of skinning, we have prepared a tailored skin for geoserver at https://apitestbed.geonovum.nl/geoserver/ogc/features. We've added a layout inspired on the GeoNovum website. For this, 2 layout template overrides have been added to ~/data/templates/ogc/features .","title":"GeoServer"},{"location":"cases/skinning/#ldproxy","text":"The HTML encoding is implemented using Mustache templates . Custom templates are supported, they have to reside in the data directory under the relative path templates/html/{templateName}.mustache , where {templateName} equals the name of a default template (see source code on GitHub) (taken from ldproxy docs ).","title":"LDProxy"},{"location":"cases/skinning/#geonetwork","text":"GeoNetwork offers an experimental OGC API Records implementation at https://github.com/geonetwork/geonetwork-microservices/tree/main/modules/services/ogc-api-records. This plugin can be installed on the latest v4 GeoNetwork. GeoNetwork uses xslt to provide a html interface. The xslt templates are located at https://github.com/geonetwork/geonetwork-microservices/tree/main/modules/services/ogc-api-records/src/main/resources/xslt/ogcapir. <div class=\"w-2/3 pr-4\"> <abstract> <xsl:value-of select=\"$abstract\"/> </abstract> </div>","title":"GeoNetwork"},{"location":"credits/","text":"Credits The OGC API Testbed Platform was initially developed in May-June 2021 by Geonovum . Developers were, alphabetically: Thijs Brentjens , also team-lead, point of contact Just van den Broecke Paul van Genuchten","title":"Credits"},{"location":"credits/#credits","text":"The OGC API Testbed Platform was initially developed in May-June 2021 by Geonovum . Developers were, alphabetically: Thijs Brentjens , also team-lead, point of contact Just van den Broecke Paul van Genuchten","title":"Credits"},{"location":"findings/","text":"Installation findings This document lists some of the experiences during installation and creation of the software, for example: what is easy to do, what not? what is supported by which software? configuration setup Docker Docker (and related Cloud-technologies like Kubernetes and Cloud Foundry) are replacing the traditional installation and maintenance of web services, often dubbed as \" Box Hugging \". These Cloud-technologies are following the Pets vs Cattle paradigm. Many pro's and con's are documented. We list a number of them which came up during the project. Pro's For setups of pygeoapi and geoserver with OGR support (used in wfs and geopackage backend) the use of Docker is attractive, due to complexity of managing dedicated dependencies. Running the full platform locally doesn't require any effort. Traefik manages the change from https://domain to http://localhost transparently. But docker is the main driver of this capability. Con's In the field there is a growing awareness that docker also has limitations. Docker images do not receieve similar efforts to keep them updated as the traditional systems. The risk of non patched vulnarabilities is higher when running a docker infrastructue. Data management A typical use case will be that a geonovum employee arrives with some shapefiles to be published. The Shapefile can be deployed as part of the deployment (via github). An alternative route is to import the data into postgres. The data can then be used in various applications. Importing a shapefile into postgres requires direct connection to postgres or use the PGAdmin dump import. GeoServer (via GeoCat Bridge) has an option to upload a shapefile and import it to Postgres. See HOWTO data on how to use both approaches. Run infra locally The current deployment can be run locally on Linux and Mac easily, which is helpful to test a new development before creating the Pull Request. However it may be the case that this does not easily work on Windows. On the other hand, maybe this is not a scenario, because the Geonovum employee might update the configuration in GitHub , and deploy it to the Sandbox environment, and use that as a test prior to moving the configuration to the production system. Use of attached storage Using attached storage is common for larger files with a focus on read access. Can we use it to store grids (tiff) or tile cache to make it accessible for various services? Using attached storage as backend for geopackage or postgres is not optimal on attached storage because of many concurrent requests and file locking. Attached storage is usefull for bulk downloads (inspire stored query). Read access to a tilecache can be usefull, but seeding is problematic, due to the number of write requests. Load balancing Current setup does not have scaling (it is possible using traeffik, but currently not set up). It could be relevant to set up load balancing, at some point because it has its own type of challenges. Auto scaling as provided by for example Kubernetes, is not in scope for this experiment. Kubernetes generally requires a large hosting farm, such as azure, google. Include geoserver in the experiment? We had some discussion if we should include GeoServer in the experiment. GeoServer is known to have challenges in cloud environments (memory usage, stability). But it is not explicitely known which challenges those are, and if their are ways to move around them. That's why it is usefull to include it. Also because the software has a high adoption at Dutch data providers. An aspect of GeoServer challenges in cloud is the complexity of its config files. The config files are designed around the web user interface which is commonly used to set them up, and heavily depends on relations identified by complex uuid strings. Running multiple geoservers along side requires to synchronise the config files over the instances. This gets extra challenging in case GeoServer has a jdbcconfig community module which allows to store the configuration in a database. But that plugin is not an official status yet (low TRL). Initiatives exist to define a geoserver cloud native strategy, based around an event bus. URL configuration Products tend to include a configuration parameter to indicate the outside url in which the service is made available. This url is for example used in a getcapabilities response to indicate the endpoint of the service. In CI/CD environments this parameter is challenging, because it may vary based on how you access the service (via internal or remote). There is actually no need to persist this in a parameter, because the value is also provided by the x-forwarded-for header of the request. Mind that the gateway software should be set up to add the x-forwarded-for header to the request. GeoServer facilitates for example this use case, in the settings you can activate a setting: 'use header for proxy url'. Traeffik caused additional challenges for the CSRF token check in geoserver. Seems you have to whitelist the proxy domain or disable csrf check at all (our choice). CSRF support has been added to recent geoserver versions, it offers an additional protection against script attacks. Log handling To set up a proper mechanism to persist and visualise (error) logs is an important aspect of a successfull SDI implementation. Logs can be evaluated to find the cause of a problem, or more general find aspects to improve on the implementation. 3 types of logs can be identified: Error logs (generated by the application) Usage logs (typically at the gateway level) (un)availability (and hardware monitoring) Important aspects to evaluate is to prevent log files to grow to unexpected size and not get destructed at redeployment. Setting up proper log rotation is key. Another aspect to consider is that log files have a GDPR (AVG) aspect. Access logs typically persist ip adresses of endusers. Error logs Within Docker it is a convention to report errors via stdout, so they are picked up by docker. LDProxy needs a dedicated configuration to set up logging to stdout. Tools like logstash are able to persist the logs from docker and visualise it in kibana. We have not implemented a central collection of error logs. Instead we delegate to portainer for viewing logs. Usage logs Traeffik needs to be set up to direct access-logs to a channel. A very basic option to persist and vizualise these logs is AWStats. More advanced tools are capable to cluster groups of requests, for example all requests within a certain bounding box or feature type. Availability logs Various generic products exist such as pingdom, checkmk, zabbix, nagios. Cloud platforms such as kubernetes have embedded systems. A dedicated product exists for the geospatial world, called GeoHealthCheck. It monitors the availability (and to a degree the complience) of gis layers. (see HOWTO ghc on how to use it) Backup Backup (or synchronisation) should be set up for volatile data, such as databases and log files. These aspects are less relevant to this infrastructure, because we persist all configuration in GitHub and loss of the log files is not very critical. We therefore have not set up any backups or data synchronisation.","title":"Findings"},{"location":"findings/#installation-findings","text":"This document lists some of the experiences during installation and creation of the software, for example: what is easy to do, what not? what is supported by which software? configuration setup","title":"Installation findings"},{"location":"findings/#docker","text":"Docker (and related Cloud-technologies like Kubernetes and Cloud Foundry) are replacing the traditional installation and maintenance of web services, often dubbed as \" Box Hugging \". These Cloud-technologies are following the Pets vs Cattle paradigm. Many pro's and con's are documented. We list a number of them which came up during the project. Pro's For setups of pygeoapi and geoserver with OGR support (used in wfs and geopackage backend) the use of Docker is attractive, due to complexity of managing dedicated dependencies. Running the full platform locally doesn't require any effort. Traefik manages the change from https://domain to http://localhost transparently. But docker is the main driver of this capability. Con's In the field there is a growing awareness that docker also has limitations. Docker images do not receieve similar efforts to keep them updated as the traditional systems. The risk of non patched vulnarabilities is higher when running a docker infrastructue.","title":"Docker"},{"location":"findings/#data-management","text":"A typical use case will be that a geonovum employee arrives with some shapefiles to be published. The Shapefile can be deployed as part of the deployment (via github). An alternative route is to import the data into postgres. The data can then be used in various applications. Importing a shapefile into postgres requires direct connection to postgres or use the PGAdmin dump import. GeoServer (via GeoCat Bridge) has an option to upload a shapefile and import it to Postgres. See HOWTO data on how to use both approaches.","title":"Data management"},{"location":"findings/#run-infra-locally","text":"The current deployment can be run locally on Linux and Mac easily, which is helpful to test a new development before creating the Pull Request. However it may be the case that this does not easily work on Windows. On the other hand, maybe this is not a scenario, because the Geonovum employee might update the configuration in GitHub , and deploy it to the Sandbox environment, and use that as a test prior to moving the configuration to the production system.","title":"Run infra locally"},{"location":"findings/#use-of-attached-storage","text":"Using attached storage is common for larger files with a focus on read access. Can we use it to store grids (tiff) or tile cache to make it accessible for various services? Using attached storage as backend for geopackage or postgres is not optimal on attached storage because of many concurrent requests and file locking. Attached storage is usefull for bulk downloads (inspire stored query). Read access to a tilecache can be usefull, but seeding is problematic, due to the number of write requests.","title":"Use of attached storage"},{"location":"findings/#load-balancing","text":"Current setup does not have scaling (it is possible using traeffik, but currently not set up). It could be relevant to set up load balancing, at some point because it has its own type of challenges. Auto scaling as provided by for example Kubernetes, is not in scope for this experiment. Kubernetes generally requires a large hosting farm, such as azure, google.","title":"Load balancing"},{"location":"findings/#include-geoserver-in-the-experiment","text":"We had some discussion if we should include GeoServer in the experiment. GeoServer is known to have challenges in cloud environments (memory usage, stability). But it is not explicitely known which challenges those are, and if their are ways to move around them. That's why it is usefull to include it. Also because the software has a high adoption at Dutch data providers. An aspect of GeoServer challenges in cloud is the complexity of its config files. The config files are designed around the web user interface which is commonly used to set them up, and heavily depends on relations identified by complex uuid strings. Running multiple geoservers along side requires to synchronise the config files over the instances. This gets extra challenging in case GeoServer has a jdbcconfig community module which allows to store the configuration in a database. But that plugin is not an official status yet (low TRL). Initiatives exist to define a geoserver cloud native strategy, based around an event bus.","title":"Include geoserver in the experiment?"},{"location":"findings/#url-configuration","text":"Products tend to include a configuration parameter to indicate the outside url in which the service is made available. This url is for example used in a getcapabilities response to indicate the endpoint of the service. In CI/CD environments this parameter is challenging, because it may vary based on how you access the service (via internal or remote). There is actually no need to persist this in a parameter, because the value is also provided by the x-forwarded-for header of the request. Mind that the gateway software should be set up to add the x-forwarded-for header to the request. GeoServer facilitates for example this use case, in the settings you can activate a setting: 'use header for proxy url'. Traeffik caused additional challenges for the CSRF token check in geoserver. Seems you have to whitelist the proxy domain or disable csrf check at all (our choice). CSRF support has been added to recent geoserver versions, it offers an additional protection against script attacks.","title":"URL configuration"},{"location":"findings/#log-handling","text":"To set up a proper mechanism to persist and visualise (error) logs is an important aspect of a successfull SDI implementation. Logs can be evaluated to find the cause of a problem, or more general find aspects to improve on the implementation. 3 types of logs can be identified: Error logs (generated by the application) Usage logs (typically at the gateway level) (un)availability (and hardware monitoring) Important aspects to evaluate is to prevent log files to grow to unexpected size and not get destructed at redeployment. Setting up proper log rotation is key. Another aspect to consider is that log files have a GDPR (AVG) aspect. Access logs typically persist ip adresses of endusers.","title":"Log handling"},{"location":"findings/#error-logs","text":"Within Docker it is a convention to report errors via stdout, so they are picked up by docker. LDProxy needs a dedicated configuration to set up logging to stdout. Tools like logstash are able to persist the logs from docker and visualise it in kibana. We have not implemented a central collection of error logs. Instead we delegate to portainer for viewing logs.","title":"Error logs"},{"location":"findings/#usage-logs","text":"Traeffik needs to be set up to direct access-logs to a channel. A very basic option to persist and vizualise these logs is AWStats. More advanced tools are capable to cluster groups of requests, for example all requests within a certain bounding box or feature type.","title":"Usage logs"},{"location":"findings/#availability-logs","text":"Various generic products exist such as pingdom, checkmk, zabbix, nagios. Cloud platforms such as kubernetes have embedded systems. A dedicated product exists for the geospatial world, called GeoHealthCheck. It monitors the availability (and to a degree the complience) of gis layers. (see HOWTO ghc on how to use it)","title":"Availability logs"},{"location":"findings/#backup","text":"Backup (or synchronisation) should be set up for volatile data, such as databases and log files. These aspects are less relevant to this infrastructure, because we persist all configuration in GitHub and loss of the log files is not very critical. We therefore have not set up any backups or data synchronisation.","title":"Backup"},{"location":"howto/","text":"HOWTOs The following sections of HOWTOs exist. HOWTO Platform The Platform section describes how to deploy and maintain your own instance of the OGC API Testbed platform. HOWTO Platform - setting up a new platform HOWTO System Maintenance - maintaining the host system HOWTO Error Analysis HOWTO Services The deployment section describes how to deploy services, like OAFeat instances. HOWTO Deploy - create or update a service HOWTO Deploy pygeoapi HOWTO Deploy GeoServer HOWTO Deploy LDProxy HOWTO Deploy pycsw HOWTO Deploy qgis HOWTO Deploy GOAF HOWTO Cases The cases section describes how to manage certain use cases. HOWTO INSPIRE in pygeoapi HOWTO API Strategie HOWTO Admin The admin section describes various supporting tools for administration and monitoring. HOWTO GeoHealthCheck - monitoring OGC web-services HOWTO Database HOWTO Portainer - monitoring Docker Containers","title":"Howto"},{"location":"howto/#howtos","text":"The following sections of HOWTOs exist.","title":"HOWTOs"},{"location":"howto/#howto-platform","text":"The Platform section describes how to deploy and maintain your own instance of the OGC API Testbed platform. HOWTO Platform - setting up a new platform HOWTO System Maintenance - maintaining the host system HOWTO Error Analysis","title":"HOWTO Platform"},{"location":"howto/#howto-services","text":"The deployment section describes how to deploy services, like OAFeat instances. HOWTO Deploy - create or update a service HOWTO Deploy pygeoapi HOWTO Deploy GeoServer HOWTO Deploy LDProxy HOWTO Deploy pycsw HOWTO Deploy qgis HOWTO Deploy GOAF","title":"HOWTO Services"},{"location":"howto/#howto-cases","text":"The cases section describes how to manage certain use cases. HOWTO INSPIRE in pygeoapi HOWTO API Strategie","title":"HOWTO Cases"},{"location":"howto/#howto-admin","text":"The admin section describes various supporting tools for administration and monitoring. HOWTO GeoHealthCheck - monitoring OGC web-services HOWTO Database HOWTO Portainer - monitoring Docker Containers","title":"HOWTO Admin"},{"location":"howto/howto_api_strategie/","text":"HOWTO set up a pygeoapi service following API Strategie","title":"HOWTO API Strategie"},{"location":"howto/howto_api_strategie/#howto-set-up-a-pygeoapi-service-following-api-strategie","text":"","title":"HOWTO set up a pygeoapi service following API Strategie"},{"location":"howto/howto_database/","text":"HOWTO Database The infrastructure has a central PostGreSQL database which can be used by various services. Managing tables A Webbased database manager (pgAdmin) has been installed at https://apitestbed.geonovum.nl/pgadmin , which can be used to verify content in tables and administed tables and users. You can also create new tables and populate it using SQL queries (generated from a local database). Uploading data to PostGreSQL from QGIS The testbed database exposes its port to the web for conveniance purposes, this is not very common in production situations. This allows QGIS to directly connect to the database. And you can use the QGIS DB manager to upload data. Open the Data source manager to be able to add the testbed database to QGIS key value service host apitestbed.geonovum.nl port 5432 database gis SSL allow user geopost pw xxxxx Open DB Manager and select the testbed database Click the 'Import Layer/File' button and complete the wizard Connecting GeoServer to the central PostGreSQL From GeoServer admin you can create a store which connects to the central database. After which you can set up feature collections originating from that store. On the stores page, create a store . Select a store of type PostGIS (not jndi). Fill in the connection details: key value host postgis port 5432 database gis schema public user geopost pw xxxxx From the layers screen, create a new layer. Select the PostGreSQL store Select the relevant table from the database. Fill in the Layer fields, at minimum calculate the bounds of the layer. Save and preview the layer Uploading data to PostGreSQL from QGIS Bridge As part of the data publication process of QGIS Bridge, you can configure the data to be stored on PostGreSQL. Two options exist (as configuration on a server connection): Bridge will send the data to GeoServer. And GeoServer will insert the data in PostGres. Bridge will connect directly to the remote PostGreSQL and insert the data","title":"HOWTO Database"},{"location":"howto/howto_database/#howto-database","text":"The infrastructure has a central PostGreSQL database which can be used by various services.","title":"HOWTO Database"},{"location":"howto/howto_database/#managing-tables","text":"A Webbased database manager (pgAdmin) has been installed at https://apitestbed.geonovum.nl/pgadmin , which can be used to verify content in tables and administed tables and users. You can also create new tables and populate it using SQL queries (generated from a local database).","title":"Managing tables"},{"location":"howto/howto_database/#uploading-data-to-postgresql-from-qgis","text":"The testbed database exposes its port to the web for conveniance purposes, this is not very common in production situations. This allows QGIS to directly connect to the database. And you can use the QGIS DB manager to upload data. Open the Data source manager to be able to add the testbed database to QGIS key value service host apitestbed.geonovum.nl port 5432 database gis SSL allow user geopost pw xxxxx Open DB Manager and select the testbed database Click the 'Import Layer/File' button and complete the wizard","title":"Uploading data to PostGreSQL from QGIS"},{"location":"howto/howto_database/#connecting-geoserver-to-the-central-postgresql","text":"From GeoServer admin you can create a store which connects to the central database. After which you can set up feature collections originating from that store. On the stores page, create a store . Select a store of type PostGIS (not jndi). Fill in the connection details: key value host postgis port 5432 database gis schema public user geopost pw xxxxx From the layers screen, create a new layer. Select the PostGreSQL store Select the relevant table from the database. Fill in the Layer fields, at minimum calculate the bounds of the layer. Save and preview the layer","title":"Connecting GeoServer to the central PostGreSQL"},{"location":"howto/howto_database/#uploading-data-to-postgresql-from-qgis-bridge","text":"As part of the data publication process of QGIS Bridge, you can configure the data to be stored on PostGreSQL. Two options exist (as configuration on a server connection): Bridge will send the data to GeoServer. And GeoServer will insert the data in PostGres. Bridge will connect directly to the remote PostGreSQL and insert the data","title":"Uploading data to PostGreSQL from QGIS Bridge"},{"location":"howto/howto_deploy/","text":"Service Deployment The API testbed environment uses a configuration mechanism stored in the GitHub repository. Whenever GitHub detects a commit in the repository, a deployment on the remote server of the changed service is triggered automatically. Such an approach is known as Continuous Deployment or \"CD\". The API Testbed uses Ansible and GitHub Workflows to enable CD. Effectively, new or changed Docker Containers and their configuration are deployed on a remote server (VM/VPS) from within GitHub. While a deployment task is running, you can follow its progress on GitHub . It is possible to directly commit your changes to GitHub, but a better practice is to work from Pull Requests often called a PR . Some discussion and an approval process can happen around a Pull Request, before it is merged and deployed. For your case, decide if you want to update an existing service or create a new service. All services in the platform are available as paths on a single domain. Each service contains an orchestration of one or more Docker Containers , which together provide the functionality of the service. Docker containers are based on of-the-shelf product images from DockerHub, combined with a service-specific configuration. Update a service Change the required files on an existing service folder. Either directly on GitHub, but preferably by cloning the repository locally and issuing a PR. Make the changes, commit, and push the changes. Create a new service Firstly determine if you can instead update a service, for example with a new Collection, somewhat similar to a Layer. For a new service the best approach is to duplicate the entire folder of an existing service and change the required parameters within that folder. NB be sure to also preserve the executable properties of the .sh (Shell) files! Assumed is that the new service is using one of the existing components, thus services for GeoServer, pygeoapi, ldproxy etc. Creating a new service is basically the following multi-step process: Step 1 - Duplicate Folder Duplicate the entire folder of an existing service and name it to the new service, say xyz . So it will reside in the folder git/services/xyz/ . Step 2 - Adapt Variables In the best case only a single line in the file git/services/xyz/env.sh needs to be adapted, i.e. the SERVICE_NAME variable. This will then automatically propagate the value for the subpath in the full service-URL plus other settings within the docker-compose.yml file. In the best case docker-compose.yml requires no changes. Step 3 - Adapt Service Config and Data File(s) This step is specific to the service-component. For example pygeoapi has a single local.config.yml file. In many cases the full service URL with the subpath needs to be adapted. Others, like GOAF, may need var-settings in the docker-compose.yml file. Usually you will add data files like GeoPackage-files on a /data/ subfolder. Step 4 - Adapt Ansible Deploy File Duplicate a service definition in the Ansible Playbook file deploy.yml . This file is under git/ansible/deploy.yml . Use the service name for the service (folder) like: - name: \"xyz\" shell: \"cd {{ services_home }}/xyz && ./deploy.sh && docker ps\" tags: xyz Step 5 - Create a GitHub Action File This is a Action/Workflow File always to be placed under .github/workflows GitHub should execute this file (for our repo) when two conditions are met: 1) a commit (direct or via a PR) to the main repository branch and 2) when the change is made to the new services/xyz folder (or a subfolder). Also here: easiest is to copy any existing service deploy file like deploy.pygeoapi.yml and make a global change e.g. \"pygeoapi\" to \"xyz\". The file should look like: name: xyz Deploy \u2699\ufe0f # Trigger only when services/xyz subdir changed on: push: paths: - 'services/xyz/**' jobs: main: runs-on: ubuntu-20.04 steps: - name: Checkout \u2705 uses: actions/checkout@v2 - name: Run playbook \u2699 uses: dawidd6/action-ansible-playbook@v2 with: playbook: deploy.yml directory: ./ansible key: ${{secrets.ANSIBLE_SSH_PRIVATE_KEY}} inventory: ${{secrets.ANSIBLE_INVENTORY_PROD}} vault_password: ${{secrets.ANSIBLE_VAULT_PASSWORD}} options: | --tags xyz --verbose Basically this file will execute the above Ansible Playbook deploy.yml for the tag xyz whenever a change is committed/pushed to the services/xyz/ folder. Testing your service You can either directly commit the service configuration in the sandbox and evaluate if it behaves properly. Alternatively you can clone the full repository locally and run the environment locally (installation of docker desktop is required) before committing. Always test your service in the sandbox environment before duplicating it to the production environment. Navigate to the git/services/ folder in the project and run ./start.sh","title":"Service Deployment"},{"location":"howto/howto_deploy/#service-deployment","text":"The API testbed environment uses a configuration mechanism stored in the GitHub repository. Whenever GitHub detects a commit in the repository, a deployment on the remote server of the changed service is triggered automatically. Such an approach is known as Continuous Deployment or \"CD\". The API Testbed uses Ansible and GitHub Workflows to enable CD. Effectively, new or changed Docker Containers and their configuration are deployed on a remote server (VM/VPS) from within GitHub. While a deployment task is running, you can follow its progress on GitHub . It is possible to directly commit your changes to GitHub, but a better practice is to work from Pull Requests often called a PR . Some discussion and an approval process can happen around a Pull Request, before it is merged and deployed. For your case, decide if you want to update an existing service or create a new service. All services in the platform are available as paths on a single domain. Each service contains an orchestration of one or more Docker Containers , which together provide the functionality of the service. Docker containers are based on of-the-shelf product images from DockerHub, combined with a service-specific configuration.","title":"Service Deployment"},{"location":"howto/howto_deploy/#update-a-service","text":"Change the required files on an existing service folder. Either directly on GitHub, but preferably by cloning the repository locally and issuing a PR. Make the changes, commit, and push the changes.","title":"Update a service"},{"location":"howto/howto_deploy/#create-a-new-service","text":"Firstly determine if you can instead update a service, for example with a new Collection, somewhat similar to a Layer. For a new service the best approach is to duplicate the entire folder of an existing service and change the required parameters within that folder. NB be sure to also preserve the executable properties of the .sh (Shell) files! Assumed is that the new service is using one of the existing components, thus services for GeoServer, pygeoapi, ldproxy etc. Creating a new service is basically the following multi-step process:","title":"Create a new service"},{"location":"howto/howto_deploy/#step-1-duplicate-folder","text":"Duplicate the entire folder of an existing service and name it to the new service, say xyz . So it will reside in the folder git/services/xyz/ .","title":"Step 1 - Duplicate Folder"},{"location":"howto/howto_deploy/#step-2-adapt-variables","text":"In the best case only a single line in the file git/services/xyz/env.sh needs to be adapted, i.e. the SERVICE_NAME variable. This will then automatically propagate the value for the subpath in the full service-URL plus other settings within the docker-compose.yml file. In the best case docker-compose.yml requires no changes.","title":"Step 2 - Adapt Variables"},{"location":"howto/howto_deploy/#step-3-adapt-service-config-and-data-files","text":"This step is specific to the service-component. For example pygeoapi has a single local.config.yml file. In many cases the full service URL with the subpath needs to be adapted. Others, like GOAF, may need var-settings in the docker-compose.yml file. Usually you will add data files like GeoPackage-files on a /data/ subfolder.","title":"Step 3 - Adapt Service Config and Data File(s)"},{"location":"howto/howto_deploy/#step-4-adapt-ansible-deploy-file","text":"Duplicate a service definition in the Ansible Playbook file deploy.yml . This file is under git/ansible/deploy.yml . Use the service name for the service (folder) like: - name: \"xyz\" shell: \"cd {{ services_home }}/xyz && ./deploy.sh && docker ps\" tags: xyz","title":"Step 4 - Adapt Ansible Deploy File"},{"location":"howto/howto_deploy/#step-5-create-a-github-action-file","text":"This is a Action/Workflow File always to be placed under .github/workflows GitHub should execute this file (for our repo) when two conditions are met: 1) a commit (direct or via a PR) to the main repository branch and 2) when the change is made to the new services/xyz folder (or a subfolder). Also here: easiest is to copy any existing service deploy file like deploy.pygeoapi.yml and make a global change e.g. \"pygeoapi\" to \"xyz\". The file should look like: name: xyz Deploy \u2699\ufe0f # Trigger only when services/xyz subdir changed on: push: paths: - 'services/xyz/**' jobs: main: runs-on: ubuntu-20.04 steps: - name: Checkout \u2705 uses: actions/checkout@v2 - name: Run playbook \u2699 uses: dawidd6/action-ansible-playbook@v2 with: playbook: deploy.yml directory: ./ansible key: ${{secrets.ANSIBLE_SSH_PRIVATE_KEY}} inventory: ${{secrets.ANSIBLE_INVENTORY_PROD}} vault_password: ${{secrets.ANSIBLE_VAULT_PASSWORD}} options: | --tags xyz --verbose Basically this file will execute the above Ansible Playbook deploy.yml for the tag xyz whenever a change is committed/pushed to the services/xyz/ folder.","title":"Step 5 - Create a GitHub Action File"},{"location":"howto/howto_deploy/#testing-your-service","text":"You can either directly commit the service configuration in the sandbox and evaluate if it behaves properly. Alternatively you can clone the full repository locally and run the environment locally (installation of docker desktop is required) before committing. Always test your service in the sandbox environment before duplicating it to the production environment. Navigate to the git/services/ folder in the project and run ./start.sh","title":"Testing your service"},{"location":"howto/howto_errors/","text":"HOWTO Analyse Errors Error analyses is a bit hidden but the platform has various options to analyse challenges. Errors during deployment When you push a configuration change to github, a github action will trigger to deploy the change to the platform. If such an action fails (the service is not available), the first step is to open github actions and select the action related to your recent push. You can view the log of the action to look for a cause of the error. If you were not able to resolve the issue with above information, you can try to look for problems inside the container, as described in the runtime error section. A next step is to run the full platform locally, as described in Run locally . And see if the problem can be resolved locally. A final step would be to roll back the change to restore the previous state of the platform. Runtime errors Run time errors, such as incidental error pages, page not found/unresponsive, etc are best anaylised via portainer . It is important to know the name of the container causing the error. This name is assigned in the docker-compose file from which the project originates. Find the relevant container in portainer. From there you can view the logs of the conatainer, restart it and even open a webbased command line client to it. Unavailability Services being unavailable can have various causes. A first place to check is GeoHealthCheck to evaluate if other services also have problems and how long the problem already exists. Usually restarting the service via portainer will resolve the issue. Else there may be a deployment problem (in a worse case scenario it is caused by the deployment of another service).","title":"HOWTO Analyse Errors"},{"location":"howto/howto_errors/#howto-analyse-errors","text":"Error analyses is a bit hidden but the platform has various options to analyse challenges.","title":"HOWTO Analyse Errors"},{"location":"howto/howto_errors/#errors-during-deployment","text":"When you push a configuration change to github, a github action will trigger to deploy the change to the platform. If such an action fails (the service is not available), the first step is to open github actions and select the action related to your recent push. You can view the log of the action to look for a cause of the error. If you were not able to resolve the issue with above information, you can try to look for problems inside the container, as described in the runtime error section. A next step is to run the full platform locally, as described in Run locally . And see if the problem can be resolved locally. A final step would be to roll back the change to restore the previous state of the platform.","title":"Errors during deployment"},{"location":"howto/howto_errors/#runtime-errors","text":"Run time errors, such as incidental error pages, page not found/unresponsive, etc are best anaylised via portainer . It is important to know the name of the container causing the error. This name is assigned in the docker-compose file from which the project originates. Find the relevant container in portainer. From there you can view the logs of the conatainer, restart it and even open a webbased command line client to it.","title":"Runtime errors"},{"location":"howto/howto_errors/#unavailability","text":"Services being unavailable can have various causes. A first place to check is GeoHealthCheck to evaluate if other services also have problems and how long the problem already exists. Usually restarting the service via portainer will resolve the issue. Else there may be a deployment problem (in a worse case scenario it is caused by the deployment of another service).","title":"Unavailability"},{"location":"howto/howto_geoserver/","text":"HOWTO GeoServer GeoServer is a commonly used application server providing webservices based on OGC standards. GeoServer provides a web interface to set up new services, including extended authorisation options. GeoServer uses a concept of workspaces to cluster a series of collections. Each workspace in GeoServer is set up as a separate OGC API Features endpoint, e.g. https://apitestbed.geonovum.nl/geoserver/{workspace}/ogc/features, although geoserver also has an endpoint with access to all collections from all workspaces. This document lists 2 approaches to set up OGC API Features services in GeoServer. Both approaches can not be combined in a single GeoServer instance. Dynamical setup GeoServer can be dynamically configured to add new services. 2 approaches are described: Via Web Administrator This HOWTO describes how you can upload data and set up a new layer on GeoServer via the Web Administrator. Most easy option to upload your data is to insert it into the PostGreSQL database using PGAdmin or QGIS DB manager. Log in to GeoServer From the Stores page, create a new store Select type PostGIS (not jndi), fill the connection details key value host postgis port 5432 database gis schema public user geopost pw xxxxx From the layers screen, create a new layer Select the PostGreSQL store and select the relevant table Fill in the various tabs (at least calculate the layer bounds) Test the layer via layer preview Via GeoCat Bridge This HOWTO describes how you can use QGIS to setup a new layer on GeoServer. For QGIS a plugin called GeoCat Bridge is available which can publish a QGIS project as a workspace on GeoServer. The Bridge plugin is available via the plugins menu. We prepared a small video about the steps involved . Scripted setup In a scripted setup the data folder of GeoServer is prepared locally and copied or mounted into the container as part of the deployment process. This setup is usefull when working with app-schema datasets ('complex GML'), which requires dedicated configuration which is not possible via the web administrator. A helpfull tool here is Hale , which has an option to export a prepared data folder for geoserver, including the pre configured app-schema configuration","title":"HOWTO GeoServer"},{"location":"howto/howto_geoserver/#howto-geoserver","text":"GeoServer is a commonly used application server providing webservices based on OGC standards. GeoServer provides a web interface to set up new services, including extended authorisation options. GeoServer uses a concept of workspaces to cluster a series of collections. Each workspace in GeoServer is set up as a separate OGC API Features endpoint, e.g. https://apitestbed.geonovum.nl/geoserver/{workspace}/ogc/features, although geoserver also has an endpoint with access to all collections from all workspaces. This document lists 2 approaches to set up OGC API Features services in GeoServer. Both approaches can not be combined in a single GeoServer instance.","title":"HOWTO GeoServer"},{"location":"howto/howto_geoserver/#dynamical-setup","text":"GeoServer can be dynamically configured to add new services. 2 approaches are described:","title":"Dynamical setup"},{"location":"howto/howto_geoserver/#via-web-administrator","text":"This HOWTO describes how you can upload data and set up a new layer on GeoServer via the Web Administrator. Most easy option to upload your data is to insert it into the PostGreSQL database using PGAdmin or QGIS DB manager. Log in to GeoServer From the Stores page, create a new store Select type PostGIS (not jndi), fill the connection details key value host postgis port 5432 database gis schema public user geopost pw xxxxx From the layers screen, create a new layer Select the PostGreSQL store and select the relevant table Fill in the various tabs (at least calculate the layer bounds) Test the layer via layer preview","title":"Via Web Administrator"},{"location":"howto/howto_geoserver/#via-geocat-bridge","text":"This HOWTO describes how you can use QGIS to setup a new layer on GeoServer. For QGIS a plugin called GeoCat Bridge is available which can publish a QGIS project as a workspace on GeoServer. The Bridge plugin is available via the plugins menu. We prepared a small video about the steps involved .","title":"Via GeoCat Bridge"},{"location":"howto/howto_geoserver/#scripted-setup","text":"In a scripted setup the data folder of GeoServer is prepared locally and copied or mounted into the container as part of the deployment process. This setup is usefull when working with app-schema datasets ('complex GML'), which requires dedicated configuration which is not possible via the web administrator. A helpfull tool here is Hale , which has an option to export a prepared data folder for geoserver, including the pre configured app-schema configuration","title":"Scripted setup"},{"location":"howto/howto_ghc/","text":"HOWTO GeoHealthCheck GeoHealthCheck (GHC) provides a monitoring service which indicates availability and compliance to the OGC API Features (OAFeat) standard. GHC Model You may want to browse the GHC Presentation Slides to get an idea what GHC is about. The GHC conceptual model comprises the entities Resources , Probes and Checks . Resource : basically the (URL) access/endpoint of your service instance. Example: a WMS Endpoint Probe : action(s) performed on a Resource , for example a WMS GetMap request Checks : test results/responses of a Probe : Is the GetMap result an image? In the GHC UI you will manage these three entities. Add Monitoring for a Service navigate to the GHC page e.g. apitestbed.geonovum.nl/ghc/ Login Click button upper right called Add+ Select \"OGC API Features (OAFeat)\" Resource Type Add your full endpoint URL This brings you into the Resource Editor for your newly registered service endpoint (called a GHC Resource ). By default, each new Resource gets a \"Capabilities Probe\" assigned which checks the overall health of your endpoint (\"does it provide a valid Capabilities/OAS file?\") In the editor you can set various parameters and additional \"Probes\" and \"Checks\". Be sure to have at least the following two Probes active: OGC API Features (OAFeat) Capabilities - Validates OAFeat endpoint landing page OGC API Features (OAFeat) Drilldown - Traverses all Collections in the endpoint validating if Features are returned The OGC API Features (OAFeat) OpenAPI Validator does a complete OAS3 JSON Schema validation on your OAS3 Endpoint JSON document. Most OAFeat implementations, except pygeoapi and ldproxy (as on June 30, 2021), fail this Probe.","title":"HOWTO GeoHealthCheck"},{"location":"howto/howto_ghc/#howto-geohealthcheck","text":"GeoHealthCheck (GHC) provides a monitoring service which indicates availability and compliance to the OGC API Features (OAFeat) standard.","title":"HOWTO GeoHealthCheck"},{"location":"howto/howto_ghc/#ghc-model","text":"You may want to browse the GHC Presentation Slides to get an idea what GHC is about. The GHC conceptual model comprises the entities Resources , Probes and Checks . Resource : basically the (URL) access/endpoint of your service instance. Example: a WMS Endpoint Probe : action(s) performed on a Resource , for example a WMS GetMap request Checks : test results/responses of a Probe : Is the GetMap result an image? In the GHC UI you will manage these three entities.","title":"GHC Model"},{"location":"howto/howto_ghc/#add-monitoring-for-a-service","text":"navigate to the GHC page e.g. apitestbed.geonovum.nl/ghc/ Login Click button upper right called Add+ Select \"OGC API Features (OAFeat)\" Resource Type Add your full endpoint URL This brings you into the Resource Editor for your newly registered service endpoint (called a GHC Resource ). By default, each new Resource gets a \"Capabilities Probe\" assigned which checks the overall health of your endpoint (\"does it provide a valid Capabilities/OAS file?\") In the editor you can set various parameters and additional \"Probes\" and \"Checks\". Be sure to have at least the following two Probes active: OGC API Features (OAFeat) Capabilities - Validates OAFeat endpoint landing page OGC API Features (OAFeat) Drilldown - Traverses all Collections in the endpoint validating if Features are returned The OGC API Features (OAFeat) OpenAPI Validator does a complete OAS3 JSON Schema validation on your OAS3 Endpoint JSON document. Most OAFeat implementations, except pygeoapi and ldproxy (as on June 30, 2021), fail this Probe.","title":"Add Monitoring for a Service"},{"location":"howto/howto_goaf/","text":"HOWTO GOAF GOAF is a OAF implementaion in Go, maintained by PDOK, originally developed as Jivan . GOAF supports a PostGres or GeoPackage backend. Which file to serve the type of file are injected as environment variables.","title":"HOWTO GOAF"},{"location":"howto/howto_goaf/#howto-goaf","text":"GOAF is a OAF implementaion in Go, maintained by PDOK, originally developed as Jivan . GOAF supports a PostGres or GeoPackage backend. Which file to serve the type of file are injected as environment variables.","title":"HOWTO GOAF"},{"location":"howto/howto_inspire/","text":"HOWTO INSPIRE & pygeoapi This HOWTO describes how to set up an INSPIRE service in pygeoapi for a Dutch INSPIRE dataset, Beschermde Gebieden - Cultuur Historie , which is exposed via a Atom download service . pygeoapi is configured using a config file. In this config file you have to add configuration for the inspire dataset. Note that one service provides access to a single dataset (having one or more feature types). Before starting verify if: Data is in a single language or multilingual Data is harmonised or as-is","title":"HOWTO INSPIRE & pygeoapi"},{"location":"howto/howto_inspire/#howto-inspire-pygeoapi","text":"This HOWTO describes how to set up an INSPIRE service in pygeoapi for a Dutch INSPIRE dataset, Beschermde Gebieden - Cultuur Historie , which is exposed via a Atom download service . pygeoapi is configured using a config file. In this config file you have to add configuration for the inspire dataset. Note that one service provides access to a single dataset (having one or more feature types). Before starting verify if: Data is in a single language or multilingual Data is harmonised or as-is","title":"HOWTO INSPIRE &amp; pygeoapi"},{"location":"howto/howto_ldproxy/","text":"HOWTO ldproxy LDProxy currently supports 2 backends, postgres and WFS. Before adding a layer, upload some data to the PostGreSQL database as described in the HOWTO Database Then open the LDProxy Manager and login as admin/ * . Create a new service Read more at","title":"HOWTO LDProxy"},{"location":"howto/howto_ldproxy/#howto-ldproxy","text":"LDProxy currently supports 2 backends, postgres and WFS. Before adding a layer, upload some data to the PostGreSQL database as described in the HOWTO Database Then open the LDProxy Manager and login as admin/ * . Create a new service Read more at","title":"HOWTO ldproxy"},{"location":"howto/howto_passwords/","text":"HOWTO passwords Current setup does not have a single sign-on solution. All services have a dedicated password. All passwords are stored encrypted on Ansible Vault and injected into containers during deployment. HOWTO extract passwords You can decrypt the passwords from the Ansible Vault using the master password, which is circulated separately. You need Python and pip to decrypt: pip install Ansible ansible-vault decrypt git/ansible/vars/vars.yml BE SURE TO NEVER CHECK-IN DECRYPTED FILES IN GITHUB!! Always be sure to encrypt after: ansible-vault encrypt git/ansible/vars/vars.yml HOWTO add or change a password TODO HOWTO reference a ansible password from YAML TODO","title":"HOWTO passwords"},{"location":"howto/howto_passwords/#howto-passwords","text":"Current setup does not have a single sign-on solution. All services have a dedicated password. All passwords are stored encrypted on Ansible Vault and injected into containers during deployment.","title":"HOWTO passwords"},{"location":"howto/howto_passwords/#howto-extract-passwords","text":"You can decrypt the passwords from the Ansible Vault using the master password, which is circulated separately. You need Python and pip to decrypt: pip install Ansible ansible-vault decrypt git/ansible/vars/vars.yml BE SURE TO NEVER CHECK-IN DECRYPTED FILES IN GITHUB!! Always be sure to encrypt after: ansible-vault encrypt git/ansible/vars/vars.yml","title":"HOWTO extract passwords"},{"location":"howto/howto_passwords/#howto-add-or-change-a-password","text":"TODO","title":"HOWTO add or change a password"},{"location":"howto/howto_passwords/#howto-reference-a-ansible-password-from-yaml","text":"TODO","title":"HOWTO reference a ansible password from YAML"},{"location":"howto/howto_platform/","text":"HOWTO Platform Describes how to setup your own instance of the OGC API Testbed platform. As a real-world example, the OGC API Sandbox (Playground) instance is presented below step-by-step. See also another example: a pygeoapi server developed by the EU JRC . 1. Ubuntu Server Info: setup or acquire a Linux Ubuntu server, minimal Ubuntu 20.4 LTS can be a VM/VPS or bare metal server, even a local VirtualBox (with Vagrant) instance Sandbox specs: 4CPU, 16RAM, 100GB but also strongly depends on the services one needs to run root access via SSH required DNS: create A-record apisandbox.geonovum.nl for IP address 109.237.219.249 OPTIONAL: DNS: if you need docs (not for Sandbox) create CNAME and set in git/docs/docs/CNAME copy your SSH public key to /root/.ssh/authorized_keys , e.g. scp ~/.ssh/id_rsa.pub root@apisandbox.geonovum.nl:.ssh/authorized_keys test login with SSH key: ssh root@apisandbox.geonovum.nl upgrade server to latest: apt-get update && apt-get -y upgrade 2. Generate GitHub Repo Create a GitHub repository from the Template repo github.com/Geonovum/ogc-api-sandbox . Creating a repository from a template is similar to forking a repository, but there are important differences: A new fork includes the entire commit history of the parent repository, while a repository created from a template starts with a single commit. Commits to a fork don't appear in your contributions graph, while commits to a repository created from a template do appear in your contribution graph. A fork can be a temporary way to contribute code to an existing project, while creating a repository from a template starts a new project quickly. Steps (see also here : login on GitHub go to github.com/Geonovum/ogc-api-testbed above file list press green button \"Use this template\" follow the steps indicated, if you want to serve docs on a separate domain indicate \"Include all branches\" 3. Prepare Local System On your local system: Install Ansible: have Python 3 (3.7 or better) installed OPTIONAL (but recommended) create a Python Virtualenv (for Ansible) install Ansible with pip install ansible 2.9.* or higher test: ansible --version - shows ansible 2.9.19 ... test: ansible-vault --version shows ansible-vault 2.9.19 ... More on Ansible below. Install Git client. 4. Clone New GitHub Repo git clone https://github.com/Geonovum/ogc-api-sandbox.git We will call the root dir of the cloned git repo on your system just git/ from here. 5. Setup Ansible Most of the configuration that is specific to your new server is stored under git/ansible/hosts (Ansible inventories) and git/ansible/vars (variables and SSH keys). Most files are encrypted with Ansible Vault . You will need to create your own (encrypted) version of these encrypted files. For many files an example file is given. Ansible Modules Called \"Roles\" these are third-party Ansible components that help with specific tasks. Install these as follows: cd git/ansible ansible-galaxy install --roles-path ./roles -r requirements.yml Ansible Hosts The hostname is crucial to services functioning. Two steps: set content of git/ansible/hosts/prod.yml (Inventory) to ogcapi: hosts: apisandbox: ansible_port: 22 ansible_host: apisandbox.geonovum.nl ansible_user: root set content of git/services/env.sh to: #!/bin/bash # Sets global env vars based on host-name # Needed for various host-dependent configs, especiallly Traefik SSL-certs. # Export and Defaults # Assume a local deployment export DEPLOY_ENV=\"local\" export TRAEFIK_SSL_ENDPOINT= export TRAEFIK_SSL_DOMAIN=\"apisandbox.geonovum.nl\" export TRAEFIK_SSL_CERT_RESOLVER= export TRAEFIK_USE_TLS=\"false\" export HOST_UID=$(id -u) export HOST_GID=$(id -g) export HOST_UID_GID=\"${HOST_UID}:${HOST_GID}\" # Set host-dependent vars case \"${HOSTNAME}\" in \"apisandbox.geonovum.nl\") DEPLOY_ENV=\"prod\" ;; \"apisandbox\") DEPLOY_ENV=\"prod\" ;; *) echo \"Default Local Host ${HOSTNAME}\" esac if [[ ${DEPLOY_ENV} = \"prod\" ]] then source /etc/environment TRAEFIK_SSL_ENDPOINT=\"https\" TRAEFIK_SSL_CERT_RESOLVER=\"le\" TRAEFIK_USE_TLS=\"true\" fi So DEPLOY_ENV=prod here is to discern with a deployment on localhost ( DEPLOY_ENV=local , where .e.g. no https/SSL is used). Create SSH Keys These are used to invoke actions on the server both from GitHub Actions (via GitHub Sercrets) and from your local Ansible setup. Plus a set of authorized_keys for the admin SSH user. cd git/ansible/vars create new SSH keypair (no password): ssh-keygen -t rsa -q -N \"\" -f gh-key.rsa Create authorized_keys Create new git/ansible/vars/authorized_keys with your public key and for others you want to give access to the admin SSH account, plus gh-key.rsa.pub . cat gh-key.rsa.pub > authorized_keys cat ~/.ssh/id.rsa.pub >> authorized_keys cat id.rsa.pub.of.joe >> authorized_keys # etc Adapt vars.yml Create new git/ansible/vars/vars.yml from example vars.example.yml in that dir. The first part of vars.yml contains generix, less-secret, values. Use variables where possible. Format is Python-Jinja2 template-like: my_ssh_pubkey_file: ~/.ssh/id_rsa.pub my_email: my@email.nl my_admin_user: the_admin_username my_admin_home: \"/home/{{ my_admin_user }}\" my_git_home: \"{{ my_admin_home }}/git\" my_github_repo: https://github.com/Geonovum/ogc-api-sandbox.git var_dir: /var/ogcapi logs_dir: \"{{ var_dir }}/log\" services_home: \"{{ my_git_home }}/services\" platform_home: \"{{ my_git_home }}/platform\" pip_install_packages: - name: docker timezone: Europe/Amsterdam ufw_open_ports: ['22', '80', '443', '5432'] The second part deals with more secret values, like usernames and passwords for services. For most services indicated below with comment after # . GHC_ denotes GeoHealthCheck (GHC) vars. If you don't use GHC you can skip those. etc_environment: PG_DB: the_db # PostGIS service PG_USER: the_user # PostGIS service PG_PASSWORD: the_pw # PostGIS service PGADMIN_EMAIL: the_user@the_user.nl # PGadmin service PGADMIN_PASSWORD: the_pw # PGadmin service GHC_SQLALCHEMY_DATABASE_URI: postgresql://the_user:the_pw@the_db:5432/the_db # PGadmin service GHC_ADMIN_USER_NAME: the_user GHC_ADMIN_USER_PASSWORD: the_pw GHC_ADMIN_USER_EMAIL: the_user@the_user.nl GHC_NOTIFICATIONS_EMAIL: the_user@the_user.com GHC_SMTP_EMAIL: the_user@the_user.com GHC_SMTP_SERVER: smtp.gmail.com GHC_SMTP_PORT: 587 GHC_SMTP_TLS: True GHC_SMTP_SSL: False GHC_SMTP_USERNAME: the_user@the_user.com GHC_SMTP_PASSWORD: the_pw Create Ansible Vault Password global replace ~/.ssh/ansible-vault/ogc-api-testbed.txt with ~/.ssh/ansible-vault/ogc-api-sandbox.txt in git/ansible/README.md create strong password store in ~/.ssh/ansible-vault/ogc-api-sandbox.txt for convenience Set GitHub Secrets Three secrets need to be set: Go to GH repo Settings|Secrets and create these three secrets ANSIBLE_INVENTORY_PROD - with value from git/ansible/hosts/prod.yml ANSIBLE_SSH_PRIVATE_KEY - with value from git/ansible/vars/gh-key.rsa ANSIBLE_VAULT_PASSWORD - value from ~/.ssh/ansible-vault/ogc-api-sandbox.txt Encrypt Ansible Files VERY IMPORTANT. UNENCRYPTED FILES SHOULD NEVER BE CHECKED IN!!! Using ansible-vault with password encrypt these: ansible-vault encrypt --vault-password-file ~/.ssh/ansible-vault/ogc-api-sandbox.txt vars.yml ansible-vault encrypt --vault-password-file ~/.ssh/ansible-vault/ogc-api-sandbox.txt gh-key.rsa ansible-vault encrypt --vault-password-file ~/.ssh/ansible-vault/ogc-api-sandbox.txt gh-key.rsa.pub ansible-vault encrypt --vault-password-file ~/.ssh/ansible-vault/ogc-api-sandbox.txt authorized_keys Globally Replace apitestbed.geonovum.nl Under git/services replace all occurrences of apitestbed.geonovum.nl with apisandbox.geonovum.nl Disable GitHub Workflows We do not want that workflows take effect immediately. So disable them temporary by renaming the dir. git mv workflows workflows.not git add . git commit -m \"disable workflows\" git push 6 Prune Repo Tree for Unneeded Services At this step you may want to delete services you don't need: rm -rf git/docs . Documentation is already maintained and available via https://apitestdocs.geonovum.nl/ for each service you want to delete, delete these 3 resources, e.g. for service xyz rm -rf git/services/xyz rm git/.github/workflows/deploy.xyz.yml in git/ansible/deploy.yml delete the three Ansible task lines with xyz name and tag. 7 Bootstrap/provision Server Moment of truth! Bootstrap (provision the server) in single playbook. ansible-playbook -v --vault-password-file ~/.ssh/ansible-vault/ogc-api-sandbox.txt bootstrap.yml -i hosts/prod.yml If all goes well, this output should be shown at end: PLAY RECAP *********************************************************************************************************** apisandbox : ok=58 changed=22 unreachable=0 failed=0 skipped=8 rescued=0 ignored=0 Observe output for errors (better is to save output in file via .. > bootstrap.log 2>&1 ). In cases of errors and after fixes, simply rerun the above Playbook. Site should be running at: https://apisandbox.geonovum.nl Check with portainer https://apisandbox.geonovum.nl/portainer/ . 8 Resolve Issues These are typical issues found and resolved: /home/oadmin/git is owned by root, change to oadmin delete (or change) CNAME git/docs/docs permissions in services/qgis/data , make datadir writeable for all: chmod 777 services/qgis/data 9. Enable GitHub Workflows Enable by renaming: git mv workflows.not workflows git add . git commit -m \"enable workflows\" git push","title":"HOWTO Platform"},{"location":"howto/howto_platform/#howto-platform","text":"Describes how to setup your own instance of the OGC API Testbed platform. As a real-world example, the OGC API Sandbox (Playground) instance is presented below step-by-step. See also another example: a pygeoapi server developed by the EU JRC .","title":"HOWTO Platform"},{"location":"howto/howto_platform/#1-ubuntu-server","text":"Info: setup or acquire a Linux Ubuntu server, minimal Ubuntu 20.4 LTS can be a VM/VPS or bare metal server, even a local VirtualBox (with Vagrant) instance Sandbox specs: 4CPU, 16RAM, 100GB but also strongly depends on the services one needs to run root access via SSH required DNS: create A-record apisandbox.geonovum.nl for IP address 109.237.219.249 OPTIONAL: DNS: if you need docs (not for Sandbox) create CNAME and set in git/docs/docs/CNAME copy your SSH public key to /root/.ssh/authorized_keys , e.g. scp ~/.ssh/id_rsa.pub root@apisandbox.geonovum.nl:.ssh/authorized_keys test login with SSH key: ssh root@apisandbox.geonovum.nl upgrade server to latest: apt-get update && apt-get -y upgrade","title":"1. Ubuntu Server"},{"location":"howto/howto_platform/#2-generate-github-repo","text":"Create a GitHub repository from the Template repo github.com/Geonovum/ogc-api-sandbox . Creating a repository from a template is similar to forking a repository, but there are important differences: A new fork includes the entire commit history of the parent repository, while a repository created from a template starts with a single commit. Commits to a fork don't appear in your contributions graph, while commits to a repository created from a template do appear in your contribution graph. A fork can be a temporary way to contribute code to an existing project, while creating a repository from a template starts a new project quickly. Steps (see also here : login on GitHub go to github.com/Geonovum/ogc-api-testbed above file list press green button \"Use this template\" follow the steps indicated, if you want to serve docs on a separate domain indicate \"Include all branches\"","title":"2. Generate GitHub Repo"},{"location":"howto/howto_platform/#3-prepare-local-system","text":"On your local system:","title":"3. Prepare Local System"},{"location":"howto/howto_platform/#install-ansible","text":"have Python 3 (3.7 or better) installed OPTIONAL (but recommended) create a Python Virtualenv (for Ansible) install Ansible with pip install ansible 2.9.* or higher test: ansible --version - shows ansible 2.9.19 ... test: ansible-vault --version shows ansible-vault 2.9.19 ... More on Ansible below.","title":"Install Ansible:"},{"location":"howto/howto_platform/#install-git-client","text":"","title":"Install Git client."},{"location":"howto/howto_platform/#4-clone-new-github-repo","text":"git clone https://github.com/Geonovum/ogc-api-sandbox.git We will call the root dir of the cloned git repo on your system just git/ from here.","title":"4. Clone New GitHub Repo"},{"location":"howto/howto_platform/#5-setup-ansible","text":"Most of the configuration that is specific to your new server is stored under git/ansible/hosts (Ansible inventories) and git/ansible/vars (variables and SSH keys). Most files are encrypted with Ansible Vault . You will need to create your own (encrypted) version of these encrypted files. For many files an example file is given.","title":"5. Setup Ansible"},{"location":"howto/howto_platform/#ansible-modules","text":"Called \"Roles\" these are third-party Ansible components that help with specific tasks. Install these as follows: cd git/ansible ansible-galaxy install --roles-path ./roles -r requirements.yml","title":"Ansible Modules"},{"location":"howto/howto_platform/#ansible-hosts","text":"The hostname is crucial to services functioning. Two steps: set content of git/ansible/hosts/prod.yml (Inventory) to ogcapi: hosts: apisandbox: ansible_port: 22 ansible_host: apisandbox.geonovum.nl ansible_user: root set content of git/services/env.sh to: #!/bin/bash # Sets global env vars based on host-name # Needed for various host-dependent configs, especiallly Traefik SSL-certs. # Export and Defaults # Assume a local deployment export DEPLOY_ENV=\"local\" export TRAEFIK_SSL_ENDPOINT= export TRAEFIK_SSL_DOMAIN=\"apisandbox.geonovum.nl\" export TRAEFIK_SSL_CERT_RESOLVER= export TRAEFIK_USE_TLS=\"false\" export HOST_UID=$(id -u) export HOST_GID=$(id -g) export HOST_UID_GID=\"${HOST_UID}:${HOST_GID}\" # Set host-dependent vars case \"${HOSTNAME}\" in \"apisandbox.geonovum.nl\") DEPLOY_ENV=\"prod\" ;; \"apisandbox\") DEPLOY_ENV=\"prod\" ;; *) echo \"Default Local Host ${HOSTNAME}\" esac if [[ ${DEPLOY_ENV} = \"prod\" ]] then source /etc/environment TRAEFIK_SSL_ENDPOINT=\"https\" TRAEFIK_SSL_CERT_RESOLVER=\"le\" TRAEFIK_USE_TLS=\"true\" fi So DEPLOY_ENV=prod here is to discern with a deployment on localhost ( DEPLOY_ENV=local , where .e.g. no https/SSL is used).","title":"Ansible Hosts"},{"location":"howto/howto_platform/#create-ssh-keys","text":"These are used to invoke actions on the server both from GitHub Actions (via GitHub Sercrets) and from your local Ansible setup. Plus a set of authorized_keys for the admin SSH user. cd git/ansible/vars create new SSH keypair (no password): ssh-keygen -t rsa -q -N \"\" -f gh-key.rsa","title":"Create SSH Keys"},{"location":"howto/howto_platform/#create-authorized_keys","text":"Create new git/ansible/vars/authorized_keys with your public key and for others you want to give access to the admin SSH account, plus gh-key.rsa.pub . cat gh-key.rsa.pub > authorized_keys cat ~/.ssh/id.rsa.pub >> authorized_keys cat id.rsa.pub.of.joe >> authorized_keys # etc","title":"Create authorized_keys"},{"location":"howto/howto_platform/#adapt-varsyml","text":"Create new git/ansible/vars/vars.yml from example vars.example.yml in that dir. The first part of vars.yml contains generix, less-secret, values. Use variables where possible. Format is Python-Jinja2 template-like: my_ssh_pubkey_file: ~/.ssh/id_rsa.pub my_email: my@email.nl my_admin_user: the_admin_username my_admin_home: \"/home/{{ my_admin_user }}\" my_git_home: \"{{ my_admin_home }}/git\" my_github_repo: https://github.com/Geonovum/ogc-api-sandbox.git var_dir: /var/ogcapi logs_dir: \"{{ var_dir }}/log\" services_home: \"{{ my_git_home }}/services\" platform_home: \"{{ my_git_home }}/platform\" pip_install_packages: - name: docker timezone: Europe/Amsterdam ufw_open_ports: ['22', '80', '443', '5432'] The second part deals with more secret values, like usernames and passwords for services. For most services indicated below with comment after # . GHC_ denotes GeoHealthCheck (GHC) vars. If you don't use GHC you can skip those. etc_environment: PG_DB: the_db # PostGIS service PG_USER: the_user # PostGIS service PG_PASSWORD: the_pw # PostGIS service PGADMIN_EMAIL: the_user@the_user.nl # PGadmin service PGADMIN_PASSWORD: the_pw # PGadmin service GHC_SQLALCHEMY_DATABASE_URI: postgresql://the_user:the_pw@the_db:5432/the_db # PGadmin service GHC_ADMIN_USER_NAME: the_user GHC_ADMIN_USER_PASSWORD: the_pw GHC_ADMIN_USER_EMAIL: the_user@the_user.nl GHC_NOTIFICATIONS_EMAIL: the_user@the_user.com GHC_SMTP_EMAIL: the_user@the_user.com GHC_SMTP_SERVER: smtp.gmail.com GHC_SMTP_PORT: 587 GHC_SMTP_TLS: True GHC_SMTP_SSL: False GHC_SMTP_USERNAME: the_user@the_user.com GHC_SMTP_PASSWORD: the_pw","title":"Adapt vars.yml"},{"location":"howto/howto_platform/#create-ansible-vault-password","text":"global replace ~/.ssh/ansible-vault/ogc-api-testbed.txt with ~/.ssh/ansible-vault/ogc-api-sandbox.txt in git/ansible/README.md create strong password store in ~/.ssh/ansible-vault/ogc-api-sandbox.txt for convenience","title":"Create Ansible Vault Password"},{"location":"howto/howto_platform/#set-github-secrets","text":"Three secrets need to be set: Go to GH repo Settings|Secrets and create these three secrets ANSIBLE_INVENTORY_PROD - with value from git/ansible/hosts/prod.yml ANSIBLE_SSH_PRIVATE_KEY - with value from git/ansible/vars/gh-key.rsa ANSIBLE_VAULT_PASSWORD - value from ~/.ssh/ansible-vault/ogc-api-sandbox.txt","title":"Set GitHub Secrets"},{"location":"howto/howto_platform/#encrypt-ansible-files","text":"VERY IMPORTANT. UNENCRYPTED FILES SHOULD NEVER BE CHECKED IN!!! Using ansible-vault with password encrypt these: ansible-vault encrypt --vault-password-file ~/.ssh/ansible-vault/ogc-api-sandbox.txt vars.yml ansible-vault encrypt --vault-password-file ~/.ssh/ansible-vault/ogc-api-sandbox.txt gh-key.rsa ansible-vault encrypt --vault-password-file ~/.ssh/ansible-vault/ogc-api-sandbox.txt gh-key.rsa.pub ansible-vault encrypt --vault-password-file ~/.ssh/ansible-vault/ogc-api-sandbox.txt authorized_keys","title":"Encrypt Ansible Files"},{"location":"howto/howto_platform/#globally-replace-apitestbedgeonovumnl","text":"Under git/services replace all occurrences of apitestbed.geonovum.nl with apisandbox.geonovum.nl","title":"Globally Replace apitestbed.geonovum.nl"},{"location":"howto/howto_platform/#disable-github-workflows","text":"We do not want that workflows take effect immediately. So disable them temporary by renaming the dir. git mv workflows workflows.not git add . git commit -m \"disable workflows\" git push","title":"Disable GitHub Workflows"},{"location":"howto/howto_platform/#6-prune-repo-tree-for-unneeded-services","text":"At this step you may want to delete services you don't need: rm -rf git/docs . Documentation is already maintained and available via https://apitestdocs.geonovum.nl/ for each service you want to delete, delete these 3 resources, e.g. for service xyz rm -rf git/services/xyz rm git/.github/workflows/deploy.xyz.yml in git/ansible/deploy.yml delete the three Ansible task lines with xyz name and tag.","title":"6 Prune Repo Tree for Unneeded Services"},{"location":"howto/howto_platform/#7-bootstrapprovision-server","text":"Moment of truth! Bootstrap (provision the server) in single playbook. ansible-playbook -v --vault-password-file ~/.ssh/ansible-vault/ogc-api-sandbox.txt bootstrap.yml -i hosts/prod.yml If all goes well, this output should be shown at end: PLAY RECAP *********************************************************************************************************** apisandbox : ok=58 changed=22 unreachable=0 failed=0 skipped=8 rescued=0 ignored=0 Observe output for errors (better is to save output in file via .. > bootstrap.log 2>&1 ). In cases of errors and after fixes, simply rerun the above Playbook. Site should be running at: https://apisandbox.geonovum.nl Check with portainer https://apisandbox.geonovum.nl/portainer/ .","title":"7 Bootstrap/provision Server"},{"location":"howto/howto_platform/#8-resolve-issues","text":"These are typical issues found and resolved: /home/oadmin/git is owned by root, change to oadmin delete (or change) CNAME git/docs/docs permissions in services/qgis/data , make datadir writeable for all: chmod 777 services/qgis/data","title":"8 Resolve Issues"},{"location":"howto/howto_platform/#9-enable-github-workflows","text":"Enable by renaming: git mv workflows.not workflows git add . git commit -m \"enable workflows\" git push","title":"9. Enable GitHub Workflows"},{"location":"howto/howto_portainer/","text":"HOWTO Portainer Portainer is a webbased tool which allows to manage running docker containers. You can view logs of a container, evaluate hardware usage, restart it or even ssh into it. Portainer is available at https://apitestbed.geonovum.nl/portainer. Credentials of portainer are circulated as part of the infrastructure credentials.","title":"HOWTO Portainer"},{"location":"howto/howto_portainer/#howto-portainer","text":"Portainer is a webbased tool which allows to manage running docker containers. You can view logs of a container, evaluate hardware usage, restart it or even ssh into it. Portainer is available at https://apitestbed.geonovum.nl/portainer. Credentials of portainer are circulated as part of the infrastructure credentials.","title":"HOWTO Portainer"},{"location":"howto/howto_pycsw/","text":"HOWTO pycsw pycsw operates on a postgres or sqlite backend. The database is configured in a configuration file , together with other common settings. Loading data pycsw has a tool to load data from a folder of files. Alternatively CSW transactions (from for example GeoCat Bridge) can be used to insert data. But transactions need to be explicitely activated and require an authentication mechanism.","title":"HOWTO pycsw"},{"location":"howto/howto_pycsw/#howto-pycsw","text":"pycsw operates on a postgres or sqlite backend. The database is configured in a configuration file , together with other common settings.","title":"HOWTO pycsw"},{"location":"howto/howto_pycsw/#loading-data","text":"pycsw has a tool to load data from a folder of files. Alternatively CSW transactions (from for example GeoCat Bridge) can be used to insert data. But transactions need to be explicitely activated and require an authentication mechanism.","title":"Loading data"},{"location":"howto/howto_pygeoapi/","text":"HOWTO pygeoapi The pygeoapi config is the place to start when configuring a new service. The file starts with some general server configuration and then presents a list of collections. Each collection has a data store configuration referencing one of the available data backends. A common data provider is the OGR/GDAL provider which gives access to a multitude of file formats. In a minimal approach you can update the current config file and add a new layer to it. Alternatively you can create a new instance by duplicating the main pygeoapi service folder under a new name and update the main ansible orchestration to add the new service. Also you have to create a new file in https://github.com/Geonovum/ogc-api-testbed/tree/main/.github/workflows, having the new name. This tells github to (re)deploy the service when changes are detected. Note that INSPIRE mandates that each dataset is exposed via a unique service endpoint and pygeoapi can only provide a single service endpoint. Duplicating the deployment is then a usual approach. Example of a pygeoapi collection lakes: # name of the collection, e.g. /collection/lakes/items type: collection title: # title, keywords and description support multilingual en: Large Lakes nl: Grote meren description: lakes of the world, public domain keywords: - lakes crs: # CRS-es supported by backend - CRS84 links: # list of links to more info, for example metadata - type: text/html rel: canonical title: information href: http://www.naturalearthdata.com/ hreflang: en-US extents: # spatial and temporal extent of the layer spatial: bbox: [-180,-90,180,90] crs: http://www.opengis.net/def/crs/OGC/1.3/CRS84 temporal: begin: 2011-11-11 end: null # or empty providers: # list of backends - type: feature # service type (e.g. features, maps, styles, records, coverages) name: GeoJSON # type of provider (see docs for available types) data: tests/data/ne_110m_lakes.geojson # link to a file (or other provider specific configuration) id_field: id # field which contains the identifier title_field: name # field which contains the title of the element (can be multilingual)","title":"HOWTO pygeoapi"},{"location":"howto/howto_pygeoapi/#howto-pygeoapi","text":"The pygeoapi config is the place to start when configuring a new service. The file starts with some general server configuration and then presents a list of collections. Each collection has a data store configuration referencing one of the available data backends. A common data provider is the OGR/GDAL provider which gives access to a multitude of file formats. In a minimal approach you can update the current config file and add a new layer to it. Alternatively you can create a new instance by duplicating the main pygeoapi service folder under a new name and update the main ansible orchestration to add the new service. Also you have to create a new file in https://github.com/Geonovum/ogc-api-testbed/tree/main/.github/workflows, having the new name. This tells github to (re)deploy the service when changes are detected. Note that INSPIRE mandates that each dataset is exposed via a unique service endpoint and pygeoapi can only provide a single service endpoint. Duplicating the deployment is then a usual approach.","title":"HOWTO pygeoapi"},{"location":"howto/howto_pygeoapi/#example-of-a-pygeoapi-collection","text":"lakes: # name of the collection, e.g. /collection/lakes/items type: collection title: # title, keywords and description support multilingual en: Large Lakes nl: Grote meren description: lakes of the world, public domain keywords: - lakes crs: # CRS-es supported by backend - CRS84 links: # list of links to more info, for example metadata - type: text/html rel: canonical title: information href: http://www.naturalearthdata.com/ hreflang: en-US extents: # spatial and temporal extent of the layer spatial: bbox: [-180,-90,180,90] crs: http://www.opengis.net/def/crs/OGC/1.3/CRS84 temporal: begin: 2011-11-11 end: null # or empty providers: # list of backends - type: feature # service type (e.g. features, maps, styles, records, coverages) name: GeoJSON # type of provider (see docs for available types) data: tests/data/ne_110m_lakes.geojson # link to a file (or other provider specific configuration) id_field: id # field which contains the identifier title_field: name # field which contains the title of the element (can be multilingual)","title":"Example of a pygeoapi collection"},{"location":"howto/howto_qgis/","text":"HOWTO QGIS server QGIS server is configured through QGIS Desktop. The project file generated with QGIS is then uploaded to the server (along with any data to serve). Start a new project in QGIS, or update an existing. Add the relevant layers to the project. For file based data, place the files in (or under) the folder where the project is stored, else QGIS may generate unresolvable paths in the project file. If you want to use a database layer, upload any data to the remote PostGreSQL. Connect your local QGIS to the remote database and add the PostGreSQL tables as layers to the project. Go to Project Properties, open the QGIS Server tab, fill in relevant fields. Make sure to check the \"publish\" checkbox in the WFS section, else no collections will be available. Save the project as .qgs file (not .qgz) and push it to Github with all related files. The project should be called project.qgs. Alternatively you can set an environment variable QGIS_PROJECT_FILE:/etc/qgisserver/my-new-project.qgs Test your service via https://apitestbed.geonovum.nl/qgis/wfs3","title":"HOWTO QGIS server"},{"location":"howto/howto_qgis/#howto-qgis-server","text":"QGIS server is configured through QGIS Desktop. The project file generated with QGIS is then uploaded to the server (along with any data to serve). Start a new project in QGIS, or update an existing. Add the relevant layers to the project. For file based data, place the files in (or under) the folder where the project is stored, else QGIS may generate unresolvable paths in the project file. If you want to use a database layer, upload any data to the remote PostGreSQL. Connect your local QGIS to the remote database and add the PostGreSQL tables as layers to the project. Go to Project Properties, open the QGIS Server tab, fill in relevant fields. Make sure to check the \"publish\" checkbox in the WFS section, else no collections will be available. Save the project as .qgs file (not .qgz) and push it to Github with all related files. The project should be called project.qgs. Alternatively you can set an environment variable QGIS_PROJECT_FILE:/etc/qgisserver/my-new-project.qgs Test your service via https://apitestbed.geonovum.nl/qgis/wfs3","title":"HOWTO QGIS server"},{"location":"howto/howto_skinning/","text":"HOWTO adjust the looks of OGC API Features Tutorials on how to change the look and feel of OGC API features. GeoServer This toturial updates the main headr and footer of GeoServer OGC API Features, other templates can be overridden in a similar way. In the data directory which is mounted into geoserver add files templates/ogc/features/common-header.ftl and templates/ogc/features/common-footer.ftl Adjust the files to your needs and push the changes to github Tip, you can preview your changes without updating the platform by adjusting the html and css directly in the web page with the browser developer panel (F12)","title":"HOWTO adjust the looks of OGC API Features"},{"location":"howto/howto_skinning/#howto-adjust-the-looks-of-ogc-api-features","text":"Tutorials on how to change the look and feel of OGC API features.","title":"HOWTO adjust the looks of OGC API Features"},{"location":"howto/howto_skinning/#geoserver","text":"This toturial updates the main headr and footer of GeoServer OGC API Features, other templates can be overridden in a similar way. In the data directory which is mounted into geoserver add files templates/ogc/features/common-header.ftl and templates/ogc/features/common-footer.ftl Adjust the files to your needs and push the changes to github Tip, you can preview your changes without updating the platform by adjusting the html and css directly in the web page with the browser developer panel (F12)","title":"GeoServer"},{"location":"howto/howto_system/","text":"HOWTO System Maintenance Once a platform instance, as in HOWTO Platform , has been installed the host system needs to be maintained. In our case the (remote) host system runs Ubuntu. A Debian/Ubuntu system is composed of software \"Packages\". These can become outdated and need to be updated. Also packages may contain security fixes. In some cases all the Docker Containers may need a restart (Service Management). Below is described how the above is organized and how these tasks are enabled for a maintainer. * NB all this applies to the host/VM system-OS, not the OS-es that run within the Docker Containers!! ~/.ssh/ansible-vault/ogc-api-sandbox.txt is the file containing your Ansible Vault password. Security Updates Automatic updates of (Ubuntu) security fixes/patches is already done automatically. During the Ansible Bootstrap phase, the Ansible Module justb4.ubuntu-base will enable automatic security updates. Details: the specific Ansible Task can be found here . Service Management All Docker containers can be started/stopped by a Ubuntu systemd service called ogcapi . The following Ansible tasks are available: ansible-playbook -v --vault-password-file ~/.ssh/ansible-vault/ogc-api-sandbox.txt service.yml -i hosts/prod.yml --tags status ansible-playbook -v --vault-password-file ~/.ssh/ansible-vault/ogc-api-sandbox.txt service.yml -i hosts/prod.yml --tags stop ansible-playbook -v --vault-password-file ~/.ssh/ansible-vault/ogc-api-sandbox.txt service.yml -i hosts/prod.yml --tags start System Management The Ubuntu \"APT\" packages can be maintained remotely with Ansible. The host system can even be rebooted remotely. The systemd service ogcapi (see Service Management) and thus all Docker Containers will be started automatically. The following Ansible tasks are available: # Update outdated Packages ansible-playbook -v --vault-password-file ~/.ssh/ansible-vault/ogc-api-sandbox.txt system.yml -i hosts/prod.yml --tags update_packages # Reboot - all services should come back up ansible-playbook -v --vault-password-file ~/.ssh/ansible-vault/ogc-api-sandbox.txt system.yml -i hosts/prod.yml --tags reboot","title":"HOWTO System Maintenance"},{"location":"howto/howto_system/#howto-system-maintenance","text":"Once a platform instance, as in HOWTO Platform , has been installed the host system needs to be maintained. In our case the (remote) host system runs Ubuntu. A Debian/Ubuntu system is composed of software \"Packages\". These can become outdated and need to be updated. Also packages may contain security fixes. In some cases all the Docker Containers may need a restart (Service Management). Below is described how the above is organized and how these tasks are enabled for a maintainer. * NB all this applies to the host/VM system-OS, not the OS-es that run within the Docker Containers!! ~/.ssh/ansible-vault/ogc-api-sandbox.txt is the file containing your Ansible Vault password.","title":"HOWTO System Maintenance"},{"location":"howto/howto_system/#security-updates","text":"Automatic updates of (Ubuntu) security fixes/patches is already done automatically. During the Ansible Bootstrap phase, the Ansible Module justb4.ubuntu-base will enable automatic security updates. Details: the specific Ansible Task can be found here .","title":"Security Updates"},{"location":"howto/howto_system/#service-management","text":"All Docker containers can be started/stopped by a Ubuntu systemd service called ogcapi . The following Ansible tasks are available: ansible-playbook -v --vault-password-file ~/.ssh/ansible-vault/ogc-api-sandbox.txt service.yml -i hosts/prod.yml --tags status ansible-playbook -v --vault-password-file ~/.ssh/ansible-vault/ogc-api-sandbox.txt service.yml -i hosts/prod.yml --tags stop ansible-playbook -v --vault-password-file ~/.ssh/ansible-vault/ogc-api-sandbox.txt service.yml -i hosts/prod.yml --tags start","title":"Service Management"},{"location":"howto/howto_system/#system-management","text":"The Ubuntu \"APT\" packages can be maintained remotely with Ansible. The host system can even be rebooted remotely. The systemd service ogcapi (see Service Management) and thus all Docker Containers will be started automatically. The following Ansible tasks are available: # Update outdated Packages ansible-playbook -v --vault-password-file ~/.ssh/ansible-vault/ogc-api-sandbox.txt system.yml -i hosts/prod.yml --tags update_packages # Reboot - all services should come back up ansible-playbook -v --vault-password-file ~/.ssh/ansible-vault/ogc-api-sandbox.txt system.yml -i hosts/prod.yml --tags reboot","title":"System Management"},{"location":"setup/","text":"Setup This section describes the setup/installation details of the platform. Platform setup This section introduces the setup of the platform and components used in the platform infrastructure. Platform setup Operational Services A number of services is deployed on the platform: pygeoapi pycsw GeoServer ldproxy QGIS Server GOAF postgis / pgadmin Admin tools Some admin tools are made available to monitor the platform. portainer GeoHealthCheck","title":"Setup"},{"location":"setup/#setup","text":"This section describes the setup/installation details of the platform.","title":"Setup"},{"location":"setup/#platform-setup","text":"This section introduces the setup of the platform and components used in the platform infrastructure. Platform setup","title":"Platform setup"},{"location":"setup/#operational-services","text":"A number of services is deployed on the platform: pygeoapi pycsw GeoServer ldproxy QGIS Server GOAF postgis / pgadmin","title":"Operational Services"},{"location":"setup/#admin-tools","text":"Some admin tools are made available to monitor the platform. portainer GeoHealthCheck","title":"Admin tools"},{"location":"setup/geoserver/","text":"Setup GeoServer A docker hub image is provided by oscarfonts is extended with OGC API plugin. The binaries of the plugin, as well as the data folder are mounted into the container. The Oscar Fonts image runs as a tomcat user, which by itself is a good practice from security prespective, but files are created on the data folder by a user unknown to the docker host, which causes problems at redeployment. We have overridden this behaviour and run as root user. The data folder is created by deploying geoserver locally, setting up the required services and commit the changes to github. You can either embed a data file inside the data folder, alternatively you can upload data to the PostGreSQL database and configure a layer on data from the database. The OGC API is available from the geoserver homepage at /ogc/features, or on the workspace endpoint /{workspace}/agc/features. OGC API Community module The GeoServer community has been involved in the OGC sprints while the standards were shaped to its current form. The implementation of OGC API currently has the form of a community plugin , which can be installed on recent versions of GeoServer. Scripted configuration vs dynamic configuration GeoServer has an extended web user interface as well as a rest api to configure the publication of datasets. These configurations are persisted as xml files in the config folder. Alternatively you can use a scripted approach to configure the server, the xml files in the conig folder are deployed as part of the deployment. Both approaches can however not easily be combined. It means you have to decide for a server if you set it up scripted or dynamically. The scripted approach is mostly used in advanced setups such as App Schema INSPIRE. Most challenging is a sequential update number which is updated with every dynamic update of the configuration via the api. The configuration with xml files is also a challenge when scaling out and load balancing GeoServer. When these files are updated by one instance, the other instances need to be synchronised. Some community plugins are available, such as jdbc-config, which enables the storage of configuration in a central database. GeoServer behind a gateway (traefik) Running GeoServer behind a gateway, which exposes geoserver at an alternative domain, requires a proxy url to be configured on GeoServer. You need to manage this in an xml file, because the admin interface (which offers an option to configure this also) doesn't work correctly if the proxy url is not correctly set up. Recent versions of GeoServer have added a CSRF protection against script attacks. This CSRF leads to unexpected results when running GeoServer via a gateway. The gateway domain needs to be whitelisted or CSRF vealidation deactivated. Read more at https://docs.geoserver.org/stable/en/user/security/webadmin/csrf.html GeoServer and docker The installation of GeoServer requires to add the OGC API plugin (check a matching version number). It is quite common that GeoServer is extended with plugins. We used the docker image provided by oscarfonts , which has a nice mechanism to place plugins (and data folder) on a mounted volume. In order to configure a new resource on GeoServer we added the required configuration files to the geonovum github repository. GeoServer also has a web interface and rest api to configure resources, but note that any resource added manually may be overwritten from github with the new deployment of the software. An alternative for manual setup us the GeoCat bridge tool, which is a typical tool to configure new resources on GeoServer from within the QGIS or ArcMAP Desktop application. Template overrides We experimented with template overrides to add the geonovum corporate identity to the OGC API endpoints. We found some interesting behaviour, which we reported to the GeoServer community. To override common-header.ftl for /collections, you need to add the override in /data/templates/ogc/features. To override it for /items you need to place it in data/workspaces. GeoPackage support GeoPackage is often mentioned as a replacement for (appschema) GML to share larger datasets efficiently. GeoServer has support for GeoPackage as output format after installing a dedicated community plugin. The plugin also requires the wps plugin to be installed. Both plugins are installed and verified that these can be used as export format for OGC API Features. Issues Some issues found during deployment Issue #22 - Permission Issue for mounted dirs: the GeoServer Container permanently changes the ownership of mounted dirs Issue #21 - OGC API Plugin: running on subpath with https does not render linked resources correctly GEOS-10125 - GeoServer currently does not add the (meta)data linksto the links section in /collections endpoint GEOS-10126 - GeoServer uses deprecated mediatype application/x-gpkg.","title":"Setup GeoServer"},{"location":"setup/geoserver/#setup-geoserver","text":"A docker hub image is provided by oscarfonts is extended with OGC API plugin. The binaries of the plugin, as well as the data folder are mounted into the container. The Oscar Fonts image runs as a tomcat user, which by itself is a good practice from security prespective, but files are created on the data folder by a user unknown to the docker host, which causes problems at redeployment. We have overridden this behaviour and run as root user. The data folder is created by deploying geoserver locally, setting up the required services and commit the changes to github. You can either embed a data file inside the data folder, alternatively you can upload data to the PostGreSQL database and configure a layer on data from the database. The OGC API is available from the geoserver homepage at /ogc/features, or on the workspace endpoint /{workspace}/agc/features.","title":"Setup GeoServer"},{"location":"setup/geoserver/#ogc-api-community-module","text":"The GeoServer community has been involved in the OGC sprints while the standards were shaped to its current form. The implementation of OGC API currently has the form of a community plugin , which can be installed on recent versions of GeoServer.","title":"OGC API Community module"},{"location":"setup/geoserver/#scripted-configuration-vs-dynamic-configuration","text":"GeoServer has an extended web user interface as well as a rest api to configure the publication of datasets. These configurations are persisted as xml files in the config folder. Alternatively you can use a scripted approach to configure the server, the xml files in the conig folder are deployed as part of the deployment. Both approaches can however not easily be combined. It means you have to decide for a server if you set it up scripted or dynamically. The scripted approach is mostly used in advanced setups such as App Schema INSPIRE. Most challenging is a sequential update number which is updated with every dynamic update of the configuration via the api. The configuration with xml files is also a challenge when scaling out and load balancing GeoServer. When these files are updated by one instance, the other instances need to be synchronised. Some community plugins are available, such as jdbc-config, which enables the storage of configuration in a central database.","title":"Scripted configuration vs dynamic configuration"},{"location":"setup/geoserver/#geoserver-behind-a-gateway-traefik","text":"Running GeoServer behind a gateway, which exposes geoserver at an alternative domain, requires a proxy url to be configured on GeoServer. You need to manage this in an xml file, because the admin interface (which offers an option to configure this also) doesn't work correctly if the proxy url is not correctly set up. Recent versions of GeoServer have added a CSRF protection against script attacks. This CSRF leads to unexpected results when running GeoServer via a gateway. The gateway domain needs to be whitelisted or CSRF vealidation deactivated. Read more at https://docs.geoserver.org/stable/en/user/security/webadmin/csrf.html","title":"GeoServer behind a gateway (traefik)"},{"location":"setup/geoserver/#geoserver-and-docker","text":"The installation of GeoServer requires to add the OGC API plugin (check a matching version number). It is quite common that GeoServer is extended with plugins. We used the docker image provided by oscarfonts , which has a nice mechanism to place plugins (and data folder) on a mounted volume. In order to configure a new resource on GeoServer we added the required configuration files to the geonovum github repository. GeoServer also has a web interface and rest api to configure resources, but note that any resource added manually may be overwritten from github with the new deployment of the software. An alternative for manual setup us the GeoCat bridge tool, which is a typical tool to configure new resources on GeoServer from within the QGIS or ArcMAP Desktop application.","title":"GeoServer and docker"},{"location":"setup/geoserver/#template-overrides","text":"We experimented with template overrides to add the geonovum corporate identity to the OGC API endpoints. We found some interesting behaviour, which we reported to the GeoServer community. To override common-header.ftl for /collections, you need to add the override in /data/templates/ogc/features. To override it for /items you need to place it in data/workspaces.","title":"Template overrides"},{"location":"setup/geoserver/#geopackage-support","text":"GeoPackage is often mentioned as a replacement for (appschema) GML to share larger datasets efficiently. GeoServer has support for GeoPackage as output format after installing a dedicated community plugin. The plugin also requires the wps plugin to be installed. Both plugins are installed and verified that these can be used as export format for OGC API Features.","title":"GeoPackage support"},{"location":"setup/geoserver/#issues","text":"Some issues found during deployment Issue #22 - Permission Issue for mounted dirs: the GeoServer Container permanently changes the ownership of mounted dirs Issue #21 - OGC API Plugin: running on subpath with https does not render linked resources correctly GEOS-10125 - GeoServer currently does not add the (meta)data linksto the links section in /collections endpoint GEOS-10126 - GeoServer uses deprecated mediatype application/x-gpkg.","title":"Issues"},{"location":"setup/ghc/","text":"GeoHealthCheck (GHC) is an availability and Quality-of-Service (QoS) monitoring solution dedicated to OGC (web-) services. GHC supports both the standard protocols like WMS, WFS, WMTS, CSW etc, APIs in general and the recent OAFeat OGC standard. To learn more, best is to follow a GHC presentation as HTML slides or video . OAFeat Support GHC supports the OGC OAFeat standard with two basic checks (called \"Probes\"): OAFeat endpoint traversal, check if all required resources/links are available full OAS schema validation Deployment GHC is part of the Admin Stack in the testbed. GeoHealthCheck has Docker Images available at DockerHub and uses a standard PostgreSQL/PostGIS database for persistence. GHC runs with three Docker containers: GHC Web Application ( ghc_web ) GHC Runner (runs the actual checks) ( ghc_runner ) GHC Postgres database stores check config and results ( ghc_db ) Configuration GHC needs quite some variables (around 31, though many defaults apply). These are all configured once in ghc.env . Many variables represent credentials like email and database configuration. These are bundled as etc_environment in and forwarded from the encrypted Ansible file vars.yml . - name: \"admin\" shell: \"cd {{ services_home }}/admin && ./deploy.sh && docker ps\" tags: admin Links docker-compose.yml - the Docker Compose file","title":"GeoHealthCheck"},{"location":"setup/ghc/#oafeat-support","text":"GHC supports the OGC OAFeat standard with two basic checks (called \"Probes\"): OAFeat endpoint traversal, check if all required resources/links are available full OAS schema validation","title":"OAFeat Support"},{"location":"setup/ghc/#deployment","text":"GHC is part of the Admin Stack in the testbed. GeoHealthCheck has Docker Images available at DockerHub and uses a standard PostgreSQL/PostGIS database for persistence. GHC runs with three Docker containers: GHC Web Application ( ghc_web ) GHC Runner (runs the actual checks) ( ghc_runner ) GHC Postgres database stores check config and results ( ghc_db )","title":"Deployment"},{"location":"setup/ghc/#configuration","text":"GHC needs quite some variables (around 31, though many defaults apply). These are all configured once in ghc.env . Many variables represent credentials like email and database configuration. These are bundled as etc_environment in and forwarded from the encrypted Ansible file vars.yml . - name: \"admin\" shell: \"cd {{ services_home }}/admin && ./deploy.sh && docker ps\" tags: admin","title":"Configuration"},{"location":"setup/ghc/#links","text":"docker-compose.yml - the Docker Compose file","title":"Links"},{"location":"setup/goaf/","text":"Setup GOAF GOAF is a OAF implementaion in Go, maintained by PDOK, originally developed as Jivan . GOAF image used from https://hub.docker.com/r/pdok/wfs-3.0, this is an older version, but PDOK has not updated yet. Alternative could be to build the image from sources. GOAF supports a PostGres or GeoPackage backend. Which file to serve the type of file are injected as environment variables. We have added the default BRT image provided by PDOK as example.","title":"Setup GOAF"},{"location":"setup/goaf/#setup-goaf","text":"GOAF is a OAF implementaion in Go, maintained by PDOK, originally developed as Jivan . GOAF image used from https://hub.docker.com/r/pdok/wfs-3.0, this is an older version, but PDOK has not updated yet. Alternative could be to build the image from sources. GOAF supports a PostGres or GeoPackage backend. Which file to serve the type of file are injected as environment variables. We have added the default BRT image provided by PDOK as example.","title":"Setup GOAF"},{"location":"setup/ldproxy/","text":"LDProxy installation experiences/hints ldproxy started out as a prototype in the GeoNovum Geo4Web testbed in 2016. Over the years LDProxy has followed the developments around OGC API and is currently the most complete implementation of the latest developments in OGC API. Installation A docker hub image is provided by Interactive Instruments . We are using the latest image provided by Interactive Instruments, which is a bit behind from the latest version on their docker repository, but that repository is not available publicly yet. We found (and reported) some issues with the latest public release, which should be solved in the main branch. We are mounting the config folder to a local volume, the configuration in the folder is taken from github. The configuration includes a config file in which logging to stdout has been set up. For this moment we only expose a feature service with a RCE WFS as backend. We intend to also add a service with a postgres backend. The data folder is created by deploying ldproxy locally, setting up the required services and commit the changes to github. You can upload data to the PostGreSQL database and configure a layer on data from the database. The OGC API's are available via /ldproxy/services. Configure logging to stdout Challenges when exposing ldprocy on a subpath Issues running ldproxy on a subpath does not provide proper URLs to internal e.g. CSS resources Issue error on accessing collection for RCE WFS Client error, HTTP status 406, Request path : MessageBodyWriter not found for media type=image/avif, type=class java.util.ArrayList, genericType=class java.util.ArrayList.","title":"LDProxy installation experiences"},{"location":"setup/ldproxy/#ldproxy-installation-experienceshints","text":"ldproxy started out as a prototype in the GeoNovum Geo4Web testbed in 2016. Over the years LDProxy has followed the developments around OGC API and is currently the most complete implementation of the latest developments in OGC API.","title":"LDProxy installation experiences/hints"},{"location":"setup/ldproxy/#installation","text":"A docker hub image is provided by Interactive Instruments . We are using the latest image provided by Interactive Instruments, which is a bit behind from the latest version on their docker repository, but that repository is not available publicly yet. We found (and reported) some issues with the latest public release, which should be solved in the main branch. We are mounting the config folder to a local volume, the configuration in the folder is taken from github. The configuration includes a config file in which logging to stdout has been set up. For this moment we only expose a feature service with a RCE WFS as backend. We intend to also add a service with a postgres backend. The data folder is created by deploying ldproxy locally, setting up the required services and commit the changes to github. You can upload data to the PostGreSQL database and configure a layer on data from the database. The OGC API's are available via /ldproxy/services. Configure logging to stdout Challenges when exposing ldprocy on a subpath","title":"Installation"},{"location":"setup/ldproxy/#issues","text":"running ldproxy on a subpath does not provide proper URLs to internal e.g. CSS resources Issue error on accessing collection for RCE WFS Client error, HTTP status 406, Request path : MessageBodyWriter not found for media type=image/avif, type=class java.util.ArrayList, genericType=class java.util.ArrayList.","title":"Issues"},{"location":"setup/platform/","text":"Platform setup The project repository contains contains components to bootstrap, configure (\"provision\") and maintain a remote deployment of an OGC API web-service stack using modern \"DevOps\" tooling. See also a presentation of the platform on OpenGeodag 2021 . Design Principles The main design principles are: starting point is an empty VPS/VM with Ubuntu and root (key) access any action on the server/VM host is performed from a client host i.e. no direct access/login to/on the server/VM is required, only for problem solving remote actions can be performed manually or triggered by GitHub Workflows all credentials (passwords, SSH-keys, etc) are secured two operational stack instances 1) production, \"Stable\" and 2) playground, \"Sandbox\" Components The components used to realize this design are: Docker \"...OS-level virtualization to deliver software in packages called containers...\" ( Wikipedia ) Docker Compose \"...a tool for defining and running multi-container Docker applications...\" Ansible \"...an open-source software provisioning tool\" ( Wikipedia ) GitHub Actions/Workflows \"...Automate, customize, and execute software development workflows in a GitHub repository...\" Traefik a frontend proxy/load-balancer and SSL (HTTPS) endpoint. The Docker-components are used to run the operational stack, i.e. the OGC API web-services and supporting services like for monitoring. Ansible is used to provision (bootstrap) both the server OS-software and the operational stack. Ansible is executed on a local client/desktop system to invoke operations on a remote server/VM. These operations are bundled in so-called Ansible Playbooks, YAML files that describe a desired server state. GitHub Actions are used to construct Workflows. These Actions will invoke these Ansible Playbooks, effectively configuring and provisioning the operational stack on a remote server/VM. Security is guaranteed by the use of Ansible-Vault and GitHub Encrypted Secrets . Traefik manages the routing of remote requests to relevant containers. Traefik listens to Docker Engine to be aware of available containers. Containers include a dedicated configuration which is picked up by traefik to enable the route. Services A number of services has been deployed within the platform, which can act as a template to add additional services. The image below shows the operational service stack with the Traefik frontend. pygeoapi - a Python server implementation of the OGC API suite of standards. pycsw - a Python server implementation of OGC API Records. GeoServer - a Java server implementation of the OGC API suite of standards. ldproxy - a Java server implementation of the OGC API suite of standards. QGIS Server - server component of QGIS with OGC OAFeat support. GOAF - OAF service developed in Go, maintained by PDOK. PostgreSQL/PostGIS - geospatial database For administration, documentation and monitoring the following components are used: mkdocs for live documentation and landing pages PGAdmin - visual PostgreSQL manager GeoHealthCheck to monitor the availability, compliance and QoS of OGC web services Portainer visual Docker monitor and manager Production and Sandbox Instance Two separate server/CM-instances are managed to provide stable/production and sandbox/playground environments. As to control changes these instances are mapped to two GitHub repositories: https://github.com/Geonovum/ogc-api-testbed for the stable/production instance, nicknamed Stable https://github.com/Geonovum/ogc-api-sandbox the playground instance, nicknamed Sandbox The Stable repo is a so called GitHub Template repo from which the Sandbox is cloned. NB initally GitHub Protected Branches were considered, but it felt that those would be less transparent and even confusing for selective access and chances of mistakes. Other Instances When other instances are known they are added here. EC JRC The European Commission Joint Research Centre (EC JRC, Ispra It.) has also used the Template GitHub repo to create a server instance with OGC Data APIs: see their landing page at jrc.map5.nl and the GitHub repo at: https://github.com/justb4/ogc-api-jrc . Selective Redeploy When changes are pushed to the repo, only the affected services are redeployed. This is effected by a combination of GitHub Actions and Ansible Playbooks as follows: each Service has a dedicated GitHub Action \"deploy\" file, e.g. deploy.pygeoapi.yml the GitHub Action \"deploy\" file contains a trigger for a push with a paths constraint, in this example: on: push: paths: - 'services/pygeoapi/**' the GH Action then calls the Ansible Playbook deploy.yml with a --tags option related to the Service, e.g. --tags pygeoapi the deploy.yml will always update the GH repo on the server VM via the pre_tasks the Ansible task indicated by the tags is then executed Security Maintaining a public repository and providing secured access to services can be a challenge. Complex solutions exist in the Docker space using Docker Secrets, /etcd service etc We tried to keep it simpler, using Ansible Vault and GitHub Secrets are the two main mechanisms used for bootstrap and deploy. The bootstrap.yml also applies various Linux hardening components like IP-blacklisting on multiple login attempt, key-only logine etc. Steps and Workflows Below is a shortened version how to setup and maintain a testbed server instance from zero. In a dedicated HOWTO all steps are expanded/described in very great detail. Prerequisites This is what you need to have available first. Access to a server/VM This implies acquiring a server/VM instance from a hosting provider. Main requirements are that server/VM runs an LTS Ubuntu (20.4 or better) and that SSL-keys are available for root access (or an admin user account with sudo-rights). Python 3 and Ansible You need a Python 3 installation and install Ansible and git (client). Clone template repo Clone from the template repo: https://github.com/Geonovum/ogc-api-testbed.git. See how to do this . Setup Ansible Adapt the files under git/ansible/vars , following the README there. Adapt the inventory file under git/ansible/hosts , following the README there. Bootstrap the server/VM \"Bootstrap\" here implies the complete provisioning of a remote server/VM that runs the operational service stack. This is a one-time manual action, but can be executed at any time as Ansible actions are idempotent. By its nature, Ansible tasks will only change the system if there is something to do. Startpoint is a fresh Ubuntu-server or VM with root access via SSH-keys (no passwords). The Ansible playbook bootstrap.yml installs the neccessary software, and hardens the server security, e.g. using fail2ban . In this step Docker and Docker Compose are installed and a Linux systemd service is run that automatically starts/stops the operational stack, also on reboots. The software for the operational stack, i.e. from this repo, is cloned on the server as well. Maintain the server/VM This step is the daily operational maintenance. The basic substeps are: make a change, e.g. add a data Collection to an OGC API OAFeat service commit/push the change to GitHub watch the triggered GitHub Actions, check for any errors observe changes via website As indicated, a dedicated HOWTO describes the above steps in very great detail.","title":"Platform setup"},{"location":"setup/platform/#platform-setup","text":"The project repository contains contains components to bootstrap, configure (\"provision\") and maintain a remote deployment of an OGC API web-service stack using modern \"DevOps\" tooling. See also a presentation of the platform on OpenGeodag 2021 .","title":"Platform setup"},{"location":"setup/platform/#design-principles","text":"The main design principles are: starting point is an empty VPS/VM with Ubuntu and root (key) access any action on the server/VM host is performed from a client host i.e. no direct access/login to/on the server/VM is required, only for problem solving remote actions can be performed manually or triggered by GitHub Workflows all credentials (passwords, SSH-keys, etc) are secured two operational stack instances 1) production, \"Stable\" and 2) playground, \"Sandbox\"","title":"Design Principles"},{"location":"setup/platform/#components","text":"The components used to realize this design are: Docker \"...OS-level virtualization to deliver software in packages called containers...\" ( Wikipedia ) Docker Compose \"...a tool for defining and running multi-container Docker applications...\" Ansible \"...an open-source software provisioning tool\" ( Wikipedia ) GitHub Actions/Workflows \"...Automate, customize, and execute software development workflows in a GitHub repository...\" Traefik a frontend proxy/load-balancer and SSL (HTTPS) endpoint. The Docker-components are used to run the operational stack, i.e. the OGC API web-services and supporting services like for monitoring. Ansible is used to provision (bootstrap) both the server OS-software and the operational stack. Ansible is executed on a local client/desktop system to invoke operations on a remote server/VM. These operations are bundled in so-called Ansible Playbooks, YAML files that describe a desired server state. GitHub Actions are used to construct Workflows. These Actions will invoke these Ansible Playbooks, effectively configuring and provisioning the operational stack on a remote server/VM. Security is guaranteed by the use of Ansible-Vault and GitHub Encrypted Secrets . Traefik manages the routing of remote requests to relevant containers. Traefik listens to Docker Engine to be aware of available containers. Containers include a dedicated configuration which is picked up by traefik to enable the route.","title":"Components"},{"location":"setup/platform/#services","text":"A number of services has been deployed within the platform, which can act as a template to add additional services. The image below shows the operational service stack with the Traefik frontend. pygeoapi - a Python server implementation of the OGC API suite of standards. pycsw - a Python server implementation of OGC API Records. GeoServer - a Java server implementation of the OGC API suite of standards. ldproxy - a Java server implementation of the OGC API suite of standards. QGIS Server - server component of QGIS with OGC OAFeat support. GOAF - OAF service developed in Go, maintained by PDOK. PostgreSQL/PostGIS - geospatial database For administration, documentation and monitoring the following components are used: mkdocs for live documentation and landing pages PGAdmin - visual PostgreSQL manager GeoHealthCheck to monitor the availability, compliance and QoS of OGC web services Portainer visual Docker monitor and manager","title":"Services"},{"location":"setup/platform/#production-and-sandbox-instance","text":"Two separate server/CM-instances are managed to provide stable/production and sandbox/playground environments. As to control changes these instances are mapped to two GitHub repositories: https://github.com/Geonovum/ogc-api-testbed for the stable/production instance, nicknamed Stable https://github.com/Geonovum/ogc-api-sandbox the playground instance, nicknamed Sandbox The Stable repo is a so called GitHub Template repo from which the Sandbox is cloned. NB initally GitHub Protected Branches were considered, but it felt that those would be less transparent and even confusing for selective access and chances of mistakes.","title":"Production and Sandbox Instance"},{"location":"setup/platform/#other-instances","text":"When other instances are known they are added here.","title":"Other Instances"},{"location":"setup/platform/#ec-jrc","text":"The European Commission Joint Research Centre (EC JRC, Ispra It.) has also used the Template GitHub repo to create a server instance with OGC Data APIs: see their landing page at jrc.map5.nl and the GitHub repo at: https://github.com/justb4/ogc-api-jrc .","title":"EC JRC"},{"location":"setup/platform/#selective-redeploy","text":"When changes are pushed to the repo, only the affected services are redeployed. This is effected by a combination of GitHub Actions and Ansible Playbooks as follows: each Service has a dedicated GitHub Action \"deploy\" file, e.g. deploy.pygeoapi.yml the GitHub Action \"deploy\" file contains a trigger for a push with a paths constraint, in this example: on: push: paths: - 'services/pygeoapi/**' the GH Action then calls the Ansible Playbook deploy.yml with a --tags option related to the Service, e.g. --tags pygeoapi the deploy.yml will always update the GH repo on the server VM via the pre_tasks the Ansible task indicated by the tags is then executed","title":"Selective Redeploy"},{"location":"setup/platform/#security","text":"Maintaining a public repository and providing secured access to services can be a challenge. Complex solutions exist in the Docker space using Docker Secrets, /etcd service etc We tried to keep it simpler, using Ansible Vault and GitHub Secrets are the two main mechanisms used for bootstrap and deploy. The bootstrap.yml also applies various Linux hardening components like IP-blacklisting on multiple login attempt, key-only logine etc.","title":"Security"},{"location":"setup/platform/#steps-and-workflows","text":"Below is a shortened version how to setup and maintain a testbed server instance from zero. In a dedicated HOWTO all steps are expanded/described in very great detail.","title":"Steps and Workflows"},{"location":"setup/platform/#prerequisites","text":"This is what you need to have available first.","title":"Prerequisites"},{"location":"setup/platform/#access-to-a-servervm","text":"This implies acquiring a server/VM instance from a hosting provider. Main requirements are that server/VM runs an LTS Ubuntu (20.4 or better) and that SSL-keys are available for root access (or an admin user account with sudo-rights).","title":"Access to a server/VM"},{"location":"setup/platform/#python-3-and-ansible","text":"You need a Python 3 installation and install Ansible and git (client).","title":"Python 3 and Ansible"},{"location":"setup/platform/#clone-template-repo","text":"Clone from the template repo: https://github.com/Geonovum/ogc-api-testbed.git. See how to do this .","title":"Clone template repo"},{"location":"setup/platform/#setup-ansible","text":"Adapt the files under git/ansible/vars , following the README there. Adapt the inventory file under git/ansible/hosts , following the README there.","title":"Setup Ansible"},{"location":"setup/platform/#bootstrap-the-servervm","text":"\"Bootstrap\" here implies the complete provisioning of a remote server/VM that runs the operational service stack. This is a one-time manual action, but can be executed at any time as Ansible actions are idempotent. By its nature, Ansible tasks will only change the system if there is something to do. Startpoint is a fresh Ubuntu-server or VM with root access via SSH-keys (no passwords). The Ansible playbook bootstrap.yml installs the neccessary software, and hardens the server security, e.g. using fail2ban . In this step Docker and Docker Compose are installed and a Linux systemd service is run that automatically starts/stops the operational stack, also on reboots. The software for the operational stack, i.e. from this repo, is cloned on the server as well.","title":"Bootstrap the server/VM"},{"location":"setup/platform/#maintain-the-servervm","text":"This step is the daily operational maintenance. The basic substeps are: make a change, e.g. add a data Collection to an OGC API OAFeat service commit/push the change to GitHub watch the triggered GitHub Actions, check for any errors observe changes via website As indicated, a dedicated HOWTO describes the above steps in very great detail.","title":"Maintain the server/VM"},{"location":"setup/portainer/","text":"Setup portainer Portainer is a comprehensive webbased tool to monitor running containers in a Docker environment. It connects to Docker enginge to be notified of changes in running containers and hardware usage. From the user interface you can view logs, restart containers, even ssh into a container. Portainer is deployed from https://hub.docker.com/r/portainer/portainer. The portainer data folder is mounted from the host. Portainer is clustered with GeoHealthCheck in a single orchestration. Portainer is available at /portainer/ . Do not skip the trailing slash.","title":"Setup portainer"},{"location":"setup/portainer/#setup-portainer","text":"Portainer is a comprehensive webbased tool to monitor running containers in a Docker environment. It connects to Docker enginge to be notified of changes in running containers and hardware usage. From the user interface you can view logs, restart containers, even ssh into a container. Portainer is deployed from https://hub.docker.com/r/portainer/portainer. The portainer data folder is mounted from the host. Portainer is clustered with GeoHealthCheck in a single orchestration. Portainer is available at /portainer/ . Do not skip the trailing slash.","title":"Setup portainer"},{"location":"setup/postgis/","text":"Setup PostGIS TO BE SUPPLIED.","title":"Setup PostGIS"},{"location":"setup/postgis/#setup-postgis","text":"TO BE SUPPLIED.","title":"Setup PostGIS"},{"location":"setup/pycsw/","text":"pycsw installation experiences/hints pycsw is a python implementation of Catalogue Service for the Web as well as OGC API Records. Installation A docker hub image is provided by geopython . The data folder and the main config file are mounted into the container. The data folder contains a datafile, alternatively you can upload data to a PostGreSQL database. The configuration file has main details such as contact information. Check the documentation to know which properties are supported. Installation on the platform has some challenges because we install the software in a subpath. You can mimic running as root via: set a stripprefix directive in compose.yml \"traefik.http.middlewares.portainer-stripprefix.stripprefix.prefixes=/pycsw\" tell pycsw to use /pycsw/py.csw as scriptname Note that /pycsw throws a 500 error for now, but /pycsw/csw.py works fine","title":"pycsw setup"},{"location":"setup/pycsw/#pycsw-installation-experienceshints","text":"pycsw is a python implementation of Catalogue Service for the Web as well as OGC API Records.","title":"pycsw installation experiences/hints"},{"location":"setup/pycsw/#installation","text":"A docker hub image is provided by geopython . The data folder and the main config file are mounted into the container. The data folder contains a datafile, alternatively you can upload data to a PostGreSQL database. The configuration file has main details such as contact information. Check the documentation to know which properties are supported. Installation on the platform has some challenges because we install the software in a subpath. You can mimic running as root via: set a stripprefix directive in compose.yml \"traefik.http.middlewares.portainer-stripprefix.stripprefix.prefixes=/pycsw\" tell pycsw to use /pycsw/py.csw as scriptname Note that /pycsw throws a 500 error for now, but /pycsw/csw.py works fine","title":"Installation"},{"location":"setup/pygeoapi/","text":"pygeoapi installation experiences/hints pygeoapi is a python implementation of the OGC API Suite of standards. Installation A docker hub image is provided by geopython . The data folder and the main config file are mounted into the container. The data folder contains the datafiles, alternatively you can upload data to the PostGreSQL database and configure a layer on data from the database. The configuration file contains references to the collections which are exposed via the OGC API's. Check the documentation to know which backends are supported. You need to set up an instance of pygeoapi for each series of collections you want to serve on an endpoint. BRT background tiles To find out if BRT background tiles hosted by PDOK can be used, we configured this service to have a map background of WMTS tiles hosted by PDOK. BRT is hosted in epsg:28992 and epsg:3857, the latter is a requirement. Configuration is set up as: map: url: https://service.pdok.nl/brt/achtergrondkaart/wmts/v2_0/standaard/EPSG:3857/{z}/{x}/{y}.png attribution: 'Map data &copy; <a href=\"https://pdok.nl\">PDOK Kadaster</a>' INSPIRE We used this installation of pygeoapi to research the INSPIRE use case. Adding several types of links as suggested by INSPIRE is doable in pygeoapi. The software doesn't help much, but also doesn't limit you in the type of links you want to add, our configuration is: links: - type: text/html rel: describedby title: Metadata as HTML href: https://www.nationaalgeoregister.nl/geonetwork/srv/metadata/45eaae76-874a-4fe1-88f4-820517e3de73 hreflang: nl - type: application/xml rel: describedby title: Metadata as iso19139 xml href: https://www.nationaalgeoregister.nl/geonetwork/srv/metadata/45eaae76-874a-4fe1-88f4-820517e3de73/formatters/xml hreflang: nl - type: text/html rel: tag title: Referentie naar het concept beschermde gebieden (inspire registry) href: http://inspire.ec.europa.eu/featureconcept/ProtectedSite hreflang: nl - type: application/gml+xml rel: enclosure title: Download volledige dataset Werelderfgoed als GML href: https://service.pdok.nl/rce/ps-ch/wfs/v1_0?request=GetFeature&service=WFS&version=1.1.0&typeName=ps-ch:rce_inspire_polygons&outputFormat=text%2Fxml%3B%20subtype%3Dgml%2F3.1.1 hreflang: nl","title":"pygeoapi setup"},{"location":"setup/pygeoapi/#pygeoapi-installation-experienceshints","text":"pygeoapi is a python implementation of the OGC API Suite of standards.","title":"pygeoapi installation experiences/hints"},{"location":"setup/pygeoapi/#installation","text":"A docker hub image is provided by geopython . The data folder and the main config file are mounted into the container. The data folder contains the datafiles, alternatively you can upload data to the PostGreSQL database and configure a layer on data from the database. The configuration file contains references to the collections which are exposed via the OGC API's. Check the documentation to know which backends are supported. You need to set up an instance of pygeoapi for each series of collections you want to serve on an endpoint.","title":"Installation"},{"location":"setup/pygeoapi/#brt-background-tiles","text":"To find out if BRT background tiles hosted by PDOK can be used, we configured this service to have a map background of WMTS tiles hosted by PDOK. BRT is hosted in epsg:28992 and epsg:3857, the latter is a requirement. Configuration is set up as: map: url: https://service.pdok.nl/brt/achtergrondkaart/wmts/v2_0/standaard/EPSG:3857/{z}/{x}/{y}.png attribution: 'Map data &copy; <a href=\"https://pdok.nl\">PDOK Kadaster</a>'","title":"BRT background tiles"},{"location":"setup/pygeoapi/#inspire","text":"We used this installation of pygeoapi to research the INSPIRE use case. Adding several types of links as suggested by INSPIRE is doable in pygeoapi. The software doesn't help much, but also doesn't limit you in the type of links you want to add, our configuration is: links: - type: text/html rel: describedby title: Metadata as HTML href: https://www.nationaalgeoregister.nl/geonetwork/srv/metadata/45eaae76-874a-4fe1-88f4-820517e3de73 hreflang: nl - type: application/xml rel: describedby title: Metadata as iso19139 xml href: https://www.nationaalgeoregister.nl/geonetwork/srv/metadata/45eaae76-874a-4fe1-88f4-820517e3de73/formatters/xml hreflang: nl - type: text/html rel: tag title: Referentie naar het concept beschermde gebieden (inspire registry) href: http://inspire.ec.europa.eu/featureconcept/ProtectedSite hreflang: nl - type: application/gml+xml rel: enclosure title: Download volledige dataset Werelderfgoed als GML href: https://service.pdok.nl/rce/ps-ch/wfs/v1_0?request=GetFeature&service=WFS&version=1.1.0&typeName=ps-ch:rce_inspire_polygons&outputFormat=text%2Fxml%3B%20subtype%3Dgml%2F3.1.1 hreflang: nl","title":"INSPIRE"},{"location":"setup/qgis/","text":"Setup QGIS Server qgis is a desktop as well as a server solution. The server solution provides typical OGC services sucha as WMS, WFS. Since recently also OGC API Features is available. Deployed docker image from https://hub.docker.com/r/camptocamp/qgis-server. Following the hints from https://www.qcooperative.net/blog/ogcapif/ and https://docs.qgis.org/testing/en/docs/server_manual/services.html#wfs3-ogc-api-features Issues I had a hard time finding in documentation what the url is to open ogc api features endpoint, it appears to be ' /qgis/wfs3 '. I assume this will likely be changed in upcoming versions. Somehow the feature collections are not loaded, although the WMS is able to display them Container Write Permission Could not use the default /etc/qgisserver/project.qgs Docker volume mapping, as the referenced GPKG files needed write access (for WAL files) in that dir. The solutions was to set QGIS_PROJECT_FILE explicitly. However it resulted in a working situation, but the project file could not be located by the service. So I restored the situation. It means we still have no write privileges in the data folder. environment: # Must override default to allow write access e.g. GPKG WALs for www-data user - QGIS_PROJECT_FILE:/myqgisserver/project.qgs #- QGIS_SERVER_LOG_LEVEL:0 #- PGSERVICEFILE:If you want to change the default of /etc/qgisserver/pg_service.conf #- QGIS_PROJECT_FILE:If you want to change the default of /etc/qgisserver/project.qgs #- MAX_REQUESTS_PER_PROCESS:The number of requests a QGIS server will serve before being restarted by apache #- QGIS_CATCH_SEGV:1 volumes: # Map data and config into container - ./data:/myqgisserver Publish layers as WFS on the project I ran into the problem that the layers were displayed on the WMS capabilities, but not as collections on ogc api features. I requested help from the qgis mailinglist, but no direct solution. Until Allesandro Pasotti pointed me on the fact that I have to activate WFS on layers before they are available via WFS and OGC API Features. You can set WFS access via project properties > QGIS Server > WFS.","title":"Setup QGIS Server"},{"location":"setup/qgis/#setup-qgis-server","text":"qgis is a desktop as well as a server solution. The server solution provides typical OGC services sucha as WMS, WFS. Since recently also OGC API Features is available. Deployed docker image from https://hub.docker.com/r/camptocamp/qgis-server. Following the hints from https://www.qcooperative.net/blog/ogcapif/ and https://docs.qgis.org/testing/en/docs/server_manual/services.html#wfs3-ogc-api-features","title":"Setup QGIS Server"},{"location":"setup/qgis/#issues","text":"I had a hard time finding in documentation what the url is to open ogc api features endpoint, it appears to be ' /qgis/wfs3 '. I assume this will likely be changed in upcoming versions. Somehow the feature collections are not loaded, although the WMS is able to display them","title":"Issues"},{"location":"setup/qgis/#container-write-permission","text":"Could not use the default /etc/qgisserver/project.qgs Docker volume mapping, as the referenced GPKG files needed write access (for WAL files) in that dir. The solutions was to set QGIS_PROJECT_FILE explicitly. However it resulted in a working situation, but the project file could not be located by the service. So I restored the situation. It means we still have no write privileges in the data folder. environment: # Must override default to allow write access e.g. GPKG WALs for www-data user - QGIS_PROJECT_FILE:/myqgisserver/project.qgs #- QGIS_SERVER_LOG_LEVEL:0 #- PGSERVICEFILE:If you want to change the default of /etc/qgisserver/pg_service.conf #- QGIS_PROJECT_FILE:If you want to change the default of /etc/qgisserver/project.qgs #- MAX_REQUESTS_PER_PROCESS:The number of requests a QGIS server will serve before being restarted by apache #- QGIS_CATCH_SEGV:1 volumes: # Map data and config into container - ./data:/myqgisserver","title":"Container Write Permission"},{"location":"setup/qgis/#publish-layers-as-wfs-on-the-project","text":"I ran into the problem that the layers were displayed on the WMS capabilities, but not as collections on ogc api features. I requested help from the qgis mailinglist, but no direct solution. Until Allesandro Pasotti pointed me on the fact that I have to activate WFS on layers before they are available via WFS and OGC API Features. You can set WFS access via project properties > QGIS Server > WFS.","title":"Publish layers as WFS on the project"}]}